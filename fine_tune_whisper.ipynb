{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6",
      "metadata": {
        "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6"
      },
      "source": [
        "# Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfa8ad5-4cdc-4512-9058-836cbbf65e1a",
      "metadata": {
        "id": "fbfa8ad5-4cdc-4512-9058-836cbbf65e1a"
      },
      "source": [
        "In this Colab, we present a step-by-step guide on how to fine-tune Whisper\n",
        "for any multilingual ASR dataset using Hugging Face 🤗 Transformers. This is a\n",
        "more \"hands-on\" version of the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper).\n",
        "For a more in-depth explanation of Whisper, the Common Voice dataset and the theory behind fine-tuning, the reader is advised to refer to the blog post."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe0d503-ae4e-4aa7-9af4-dbcba52db41e",
      "metadata": {
        "id": "afe0d503-ae4e-4aa7-9af4-dbcba52db41e"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae91ed4-9c3e-4ade-938e-f4c2dcfbfdc0",
      "metadata": {
        "id": "9ae91ed4-9c3e-4ade-938e-f4c2dcfbfdc0"
      },
      "source": [
        "Whisper is a pre-trained model for automatic speech recognition (ASR)\n",
        "published in [September 2022](https://openai.com/blog/whisper/) by the authors\n",
        "Alec Radford et al. from OpenAI. Unlike many of its predecessors, such as\n",
        "[Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), which are pre-trained\n",
        "on un-labelled audio data, Whisper is pre-trained on a vast quantity of\n",
        "**labelled** audio-transcription data, 680,000 hours to be precise.\n",
        "This is an order of magnitude more data than the un-labelled audio data used\n",
        "to train Wav2Vec 2.0 (60,000 hours). What is more, 117,000 hours of this\n",
        "pre-training data is multilingual ASR data. This results in checkpoints\n",
        "that can be applied to over 96 languages, many of which are considered\n",
        "_low-resource_.\n",
        "\n",
        "When scaled to 680,000 hours of labelled pre-training data, Whisper models\n",
        "demonstrate a strong ability to generalise to many datasets and domains.\n",
        "The pre-trained checkpoints achieve competitive results to state-of-the-art\n",
        "ASR systems, with near 3% word error rate (WER) on the test-clean subset of\n",
        "LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._\n",
        "Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).\n",
        "The extensive multilingual ASR knowledge acquired by Whisper during pre-training\n",
        "can be leveraged for other low-resource languages; through fine-tuning, the\n",
        "pre-trained checkpoints can be adapted for specific datasets and languages\n",
        "to further improve upon these results. We'll show just how Whisper can be fine-tuned\n",
        "for low-resource languages in this Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21b6316e-8a55-4549-a154-66d3da2ab74a",
      "metadata": {
        "id": "21b6316e-8a55-4549-a154-66d3da2ab74a"
      },
      "source": [
        "The Whisper checkpoints come in five configurations of varying model sizes.\n",
        "The smallest four are trained on either English-only or multilingual data.\n",
        "The largest checkpoint is multilingual only. All nine of the pre-trained checkpoints\n",
        "are available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The\n",
        "checkpoints are summarised in the following table with links to the models on the Hub:\n",
        "\n",
        "| Size   | Layers | Width | Heads | Parameters | English-only                                         | Multilingual                                      |\n",
        "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
        "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
        "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
        "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
        "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
        "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
        "\n",
        "For demonstration purposes, we'll fine-tune the multilingual version of the\n",
        "[`\"small\"`](https://huggingface.co/openai/whisper-small) checkpoint with 244M params (~= 1GB).\n",
        "As for our data, we'll train and evaluate our system on a low-resource language\n",
        "taken from the [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)\n",
        "dataset. We'll show that with as little as 8 hours of fine-tuning data, we can achieve\n",
        "strong performance in this language."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a680dfc-cbba-4f6c-8a1f-e1a5ff3f123a",
      "metadata": {
        "id": "3a680dfc-cbba-4f6c-8a1f-e1a5ff3f123a"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "\\\\({}^1\\\\) The name Whisper follows from the acronym “WSPSR”, which stands for “Web-scale Supervised Pre-training for Speech Recognition”."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55fb8d21-df06-472a-99dd-b59567be6dad",
      "metadata": {
        "id": "55fb8d21-df06-472a-99dd-b59567be6dad"
      },
      "source": [
        "## Prepare Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "844a4861-929c-4762-b29b-80b1e95aba4b",
      "metadata": {
        "id": "844a4861-929c-4762-b29b-80b1e95aba4b"
      },
      "source": [
        "First of all, let's try to secure a decent GPU for our Colab! Unfortunately, it's becoming much harder to get access to a good GPU with the free version of Google Colab. However, with Google Colab Pro one should have no issues in being allocated a V100 or P100 GPU.\n",
        "\n",
        "To get a GPU, click _Runtime_ -> _Change runtime type_, then change _Hardware accelerator_ from _None_ to _GPU_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a",
      "metadata": {
        "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a"
      },
      "source": [
        "We can verify that we've been assigned a GPU and view its specifications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "95048026-a3b7-43f0-a274-1bad65e407b4",
      "metadata": {
        "id": "95048026-a3b7-43f0-a274-1bad65e407b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Aug 21 10:17:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  Off |\n",
            "|  0%   40C    P8    35W / 450W |    530MiB / 24564MiB |     12%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1446      G   /usr/lib/xorg/Xorg                155MiB |\n",
            "|    0   N/A  N/A      2118      G   /usr/bin/gnome-shell               83MiB |\n",
            "|    0   N/A  N/A      4527      G   ...587173396459472978,262144       87MiB |\n",
            "|    0   N/A  N/A      5582      G   ...veSuggestionsOnlyOnDemand       52MiB |\n",
            "|    0   N/A  N/A     69088      G   ...RendererForSitePerProcess      147MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d",
      "metadata": {
        "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d"
      },
      "source": [
        "We'll employ several popular Python packages to fine-tune the Whisper model.\n",
        "We'll use `datasets` to download and prepare our training data and\n",
        "`transformers` to load and train our Whisper model. We'll also require\n",
        "the `soundfile` package to pre-process audio files, `evaluate` and `jiwer` to\n",
        "assess the performance of our model. Finally, we'll\n",
        "use `gradio` to build a flashy demo of our fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e68ea9f8-9b61-414e-8885-3033b67c2850",
      "metadata": {
        "id": "e68ea9f8-9b61-414e-8885-3033b67c2850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: 2.6.1 not found\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nsw8vjpj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-nsw8vjpj\n",
            "  Resolved https://github.com/huggingface/transformers to commit 1982dd3b15867c46e1c20645901b0de469fd935f\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Using cached regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (23.1)\n",
            "Collecting safetensors>=0.3.1\n",
            "  Using cached safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1\n",
            "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "Collecting numpy>=1.17\n",
            "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "Collecting typing-extensions>=3.7.4.3\n",
            "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for transformers: filename=transformers-4.32.0.dev0-py3-none-any.whl size=7522589 sha256=8e418bd81b4601ba121bfc19fda754287268ce2631c9a39fc281fe510f49dfda\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xh8h6ivp/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, safetensors, urllib3, typing-extensions, tqdm, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
            "Successfully installed certifi-2023.7.22 charset-normalizer-3.2.0 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.16.4 idna-3.4 numpy-1.25.2 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 safetensors-0.3.2 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.32.0.dev0 typing-extensions-4.7.1 urllib3-2.0.4\n",
            "Collecting librosa\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./.env/lib/python3.10/site-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in ./.env/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
            "Collecting numba>=0.51.0\n",
            "  Using cached numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "Collecting msgpack>=1.0\n",
            "  Using cached msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
            "Collecting pooch>=1.0\n",
            "  Downloading pooch-1.7.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m299.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.2.0\n",
            "  Downloading scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting soxr>=0.3.2\n",
            "  Using cached soxr-0.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting soundfile>=0.12.1\n",
            "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in ./.env/lib/python3.10/site-packages (from librosa) (4.7.1)\n",
            "Collecting joblib>=0.14\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Collecting audioread>=2.1.9\n",
            "  Using cached audioread-3.0.0.tar.gz (377 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting scikit-learn>=0.20.0\n",
            "  Using cached scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Collecting lazy-loader>=0.1\n",
            "  Using cached lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
            "Collecting llvmlite<0.41,>=0.40.0dev0\n",
            "  Using cached llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
            "Collecting numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3\n",
            "  Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from pooch>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in ./.env/lib/python3.10/site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting cffi>=1.0\n",
            "  Using cached cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
            "Using legacy 'setup.py install' for audioread, since package 'wheel' is not installed.\n",
            "Installing collected packages: msgpack, threadpoolctl, pycparser, numpy, llvmlite, lazy-loader, joblib, audioread, soxr, scipy, pooch, numba, cffi, soundfile, scikit-learn, librosa\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Running setup.py install for audioread ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed audioread-3.0.0 cffi-1.15.1 joblib-1.3.2 lazy-loader-0.3 librosa-0.10.1 llvmlite-0.40.1 msgpack-1.0.5 numba-0.57.1 numpy-1.24.4 pooch-1.7.0 pycparser-2.21 scikit-learn-1.3.0 scipy-1.11.2 soundfile-0.12.1 soxr-0.3.6 threadpoolctl-3.2.0\n",
            "zsh:1: 0.30 not found\n",
            "Collecting jiwer\n",
            "  Using cached jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
            "Collecting click<9.0.0,>=8.1.3\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz==2.13.7\n",
            "  Using cached rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "Installing collected packages: rapidfuzz, click, jiwer\n",
            "Successfully installed click-8.1.7 jiwer-3.0.2 rapidfuzz-2.13.7\n",
            "Collecting gradio\n",
            "  Using cached gradio-3.40.1-py3-none-any.whl (20.0 MB)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Using cached mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in ./.env/lib/python3.10/site-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from gradio) (23.1)\n",
            "Collecting markupsafe~=2.0\n",
            "  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting jinja2<4.0\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting aiohttp~=3.0\n",
            "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Collecting pillow<11.0,>=8.0\n",
            "  Using cached Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.env/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
            "Collecting httpx\n",
            "  Using cached httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "Collecting ffmpy\n",
            "  Using cached ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting semantic-version~=2.0\n",
            "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting matplotlib~=3.0\n",
            "  Using cached matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "Collecting python-multipart\n",
            "  Using cached python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in ./.env/lib/python3.10/site-packages (from gradio) (4.7.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4\n",
            "  Downloading pydantic-2.2.1-py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.4/373.4 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pandas<3.0,>=1.0\n",
            "  Using cached pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "Collecting pydub\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting importlib-resources<7.0,>=1.3\n",
            "  Using cached importlib_resources-6.0.1-py3-none-any.whl (34 kB)\n",
            "Collecting gradio-client>=0.4.0\n",
            "  Using cached gradio_client-0.4.0-py3-none-any.whl (297 kB)\n",
            "Collecting altair<6.0,>=4.2.0\n",
            "  Using cached altair-5.0.1-py3-none-any.whl (471 kB)\n",
            "Collecting uvicorn>=0.14.0\n",
            "  Using cached uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "Collecting fastapi\n",
            "  Using cached fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
            "Collecting orjson~=3.0\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests~=2.0 in ./.env/lib/python3.10/site-packages (from gradio) (2.31.0)\n",
            "Collecting websockets<12.0,>=10.0\n",
            "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Requirement already satisfied: numpy~=1.0 in ./.env/lib/python3.10/site-packages (from gradio) (1.24.4)\n",
            "Collecting aiofiles<24.0,>=22.0\n",
            "  Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.env/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (3.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting toolz\n",
            "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
            "Collecting jsonschema>=3.0\n",
            "  Using cached jsonschema-4.19.0-py3-none-any.whl (83 kB)\n",
            "Requirement already satisfied: fsspec in ./.env/lib/python3.10/site-packages (from gradio-client>=0.4.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Using cached linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Using cached contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Collecting pyparsing<3.1,>=2.3.1\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Using cached mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "  Using cached mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "  Using cached mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "  Using cached mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "  Using cached mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "  Using cached mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Using cached mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "  Using cached mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting matplotlib~=3.0\n",
            "  Using cached matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "INFO: pip is looking at multiple versions of markupsafe to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting markupsafe~=2.0\n",
            "  Using cached MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "INFO: pip is looking at multiple versions of markdown-it-py[linkify] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting markdown-it-py[linkify]>=2.0.0\n",
            "  Using cached markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "Collecting tzdata>=2022.1\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.6.1\n",
            "  Downloading pydantic_core-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests~=2.0->gradio) (2.0.4)\n",
            "Collecting h11>=0.8\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Requirement already satisfied: click>=7.0 in ./.env/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting starlette<0.28.0,>=0.27.0\n",
            "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "Collecting httpcore<0.18.0,>=0.15.0\n",
            "  Using cached httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "Collecting sniffio\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting anyio<5.0,>=3.0\n",
            "  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6\n",
            "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
            "Collecting rpds-py>=0.7.1\n",
            "  Using cached rpds_py-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting referencing>=0.28.4\n",
            "  Using cached referencing-0.30.2-py3-none-any.whl (25 kB)\n",
            "Collecting uc-micro-py\n",
            "  Using cached uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Collecting exceptiongroup\n",
            "  Using cached exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Using legacy 'setup.py install' for ffmpy, since package 'wheel' is not installed.\n",
            "Installing collected packages: pytz, pydub, ffmpy, websockets, uc-micro-py, tzdata, toolz, sniffio, semantic-version, rpds-py, python-multipart, pyparsing, pydantic-core, pillow, orjson, multidict, mdurl, markupsafe, kiwisolver, importlib-resources, h11, frozenlist, fonttools, exceptiongroup, cycler, contourpy, attrs, async-timeout, annotated-types, aiofiles, yarl, uvicorn, referencing, pydantic, pandas, matplotlib, markdown-it-py, linkify-it-py, jinja2, anyio, aiosignal, starlette, mdit-py-plugins, jsonschema-specifications, httpcore, aiohttp, jsonschema, httpx, fastapi, gradio-client, altair, gradio\n",
            "  Running setup.py install for ffmpy ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed aiofiles-23.2.1 aiohttp-3.8.5 aiosignal-1.3.1 altair-5.0.1 annotated-types-0.5.0 anyio-3.7.1 async-timeout-4.0.3 attrs-23.1.0 contourpy-1.1.0 cycler-0.11.0 exceptiongroup-1.1.3 fastapi-0.101.1 ffmpy-0.3.1 fonttools-4.42.1 frozenlist-1.4.0 gradio-3.40.1 gradio-client-0.4.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 importlib-resources-6.0.1 jinja2-3.1.2 jsonschema-4.19.0 jsonschema-specifications-2023.7.1 kiwisolver-1.4.4 linkify-it-py-2.0.2 markdown-it-py-2.2.0 markupsafe-2.1.3 matplotlib-3.7.2 mdit-py-plugins-0.3.3 mdurl-0.1.2 multidict-6.0.4 orjson-3.9.5 pandas-2.0.3 pillow-10.0.0 pydantic-2.2.1 pydantic-core-2.6.1 pydub-0.25.1 pyparsing-3.0.9 python-multipart-0.0.6 pytz-2023.3 referencing-0.30.2 rpds-py-0.9.2 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.27.0 toolz-0.12.0 tzdata-2023.3 uc-micro-py-1.0.2 uvicorn-0.23.2 websockets-11.0.3 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "# !pip  install datasets\n",
        "# !pip install torch\n",
        "!pip install accelerate -U\n",
        "# !pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203",
      "metadata": {
        "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203"
      },
      "source": [
        "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
        "whilst training. The Hub provides:\n",
        "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
        "- Tensorboard logs: track important metrics over the course of training.\n",
        "- Model cards: document what a model does and its intended use cases.\n",
        "- Community: an easy way to share and collaborate with the community!\n",
        "\n",
        "Linking the notebook to the Hub is straightforward - it simply requires entering your\n",
        "Hub authentication token when prompted. Find your Hub authentication token [here](https://huggingface.co/settings/tokens):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0ad9c753",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/huggingface_hub\n",
            "  Cloning https://github.com/huggingface/huggingface_hub to /tmp/pip-req-build-2uwjeu6j\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/huggingface_hub /tmp/pip-req-build-2uwjeu6j\n",
            "  Resolved https://github.com/huggingface/huggingface_hub to commit b9901bf71839850c10ea42162c69aecabc96a46b\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: fsspec in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (3.2.0)\n",
            "Collecting ipywidgets\n",
            "  Using cached ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
            "Collecting widgetsnbextension~=4.0.7\n",
            "  Using cached widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in ./.env/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
            "Collecting jupyterlab-widgets~=3.0.7\n",
            "  Using cached jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in ./.env/lib/python3.10/site-packages (from ipywidgets) (8.14.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in ./.env/lib/python3.10/site-packages (from ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
            "Requirement already satisfied: matplotlib-inline in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
            "Requirement already satisfied: stack-data in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: pickleshare in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: decorator in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: backcall in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.env/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.env/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in ./.env/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: pure-eval in ./.env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.0 jupyterlab-widgets-3.0.8 widgetsnbextension-4.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/huggingface_hub\n",
        "!pip install wandb -qU\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b045a39e-2a3e-4153-bdb5-281500bcd348",
      "metadata": {
        "id": "b045a39e-2a3e-4153-bdb5-281500bcd348"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f443a95538184e959b08fce9659e7e7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# 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\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "63fa31ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgaychetenzin\u001b[0m (\u001b[33mmonlam-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "# 879f84539f271b958e4068c102c776ed213da057\n",
        "# 9dec6d097d098e00a8965ecee7a1b22640a6788e\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cca143d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
            "               [--paths] [--json] [--debug]\n",
            "               [subcommand]\n",
            "\n",
            "Jupyter: Interactive Computing\n",
            "\n",
            "positional arguments:\n",
            "  subcommand     the subcommand to launch\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --version      show the versions of core jupyter packages and exit\n",
            "  --config-dir   show Jupyter config dir\n",
            "  --data-dir     show Jupyter data dir\n",
            "  --runtime-dir  show Jupyter runtime dir\n",
            "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
            "                 format.\n",
            "  --json         output paths as machine-readable json\n",
            "  --debug        output debug information about paths\n",
            "\n",
            "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
            "\n",
            "Jupyter command `jupyter-notebook` not found.\n",
            "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
            "               [--paths] [--json] [--debug]\n",
            "               [subcommand]\n",
            "\n",
            "Jupyter: Interactive Computing\n",
            "\n",
            "positional arguments:\n",
            "  subcommand     the subcommand to launch\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --version      show the versions of core jupyter packages and exit\n",
            "  --config-dir   show Jupyter config dir\n",
            "  --data-dir     show Jupyter data dir\n",
            "  --runtime-dir  show Jupyter runtime dir\n",
            "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
            "                 format.\n",
            "  --json         output paths as machine-readable json\n",
            "  --debug        output debug information about paths\n",
            "\n",
            "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
            "\n",
            "Jupyter command `jupyter-notebook` not found.\n",
            "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
            "               [--paths] [--json] [--debug]\n",
            "               [subcommand]\n",
            "\n",
            "Jupyter: Interactive Computing\n",
            "\n",
            "positional arguments:\n",
            "  subcommand     the subcommand to launch\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --version      show the versions of core jupyter packages and exit\n",
            "  --config-dir   show Jupyter config dir\n",
            "  --data-dir     show Jupyter data dir\n",
            "  --runtime-dir  show Jupyter runtime dir\n",
            "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
            "                 format.\n",
            "  --json         output paths as machine-readable json\n",
            "  --debug        output debug information about paths\n",
            "\n",
            "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
            "\n",
            "Jupyter command `jupyter-notebook` not found.\n"
          ]
        }
      ],
      "source": [
        "#https://towardsdatascience.com/leveraging-the-power-of-jupyter-notebooks-26b4b8d7c622\n",
        "!jupyter notebook --generate-config\n",
        "!jupyter notebook --NotebookApp.max_buffer_size=258000000000\n",
        "!jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0",
      "metadata": {
        "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674429c5-0ab4-4adf-975b-621bb69eca38",
      "metadata": {
        "id": "674429c5-0ab4-4adf-975b-621bb69eca38"
      },
      "source": [
        "Using 🤗 Datasets, downloading and preparing data is extremely simple.\n",
        "We can download and prepare the Common Voice splits in just one line of code.\n",
        "\n",
        "First, ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally.\n",
        "\n",
        "Since Hindi is very low-resource, we'll combine the `train` and `validation`\n",
        "splits to give approximately 8 hours of training data. We'll use the 4 hours\n",
        "of `test` data as our held-out test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "51c02e2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ed601493",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 96.6M  100 96.6M    0     0  7377k      0  0:00:13  0:00:13 --:--:-- 7105k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5481k  100 5481k    0     0  8200k      0 --:--:-- --:--:-- --:--:-- 8205k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5484k  100 5484k    0     0  9052k      0 --:--:-- --:--:-- --:--:-- 9065k\n"
          ]
        }
      ],
      "source": [
        "!curl https://d38pmlk0v88drf.cloudfront.net/tsv/550-train.tsv --output tsv/train.tsv\n",
        "# !curl https://d38pmlk0v88drf.cloudfront.net/tsv/550-test.tsv --output tsv/test.tsv\n",
        "!curl https://d38pmlk0v88drf.cloudfront.net/tsv/550-val.tsv --output tsv/validation.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "42177717",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('tsv/train.tsv', sep='\\t')\n",
        "validation_df = pd.read_csv('tsv/validation.tsv', sep='\\t')\n",
        "# test_df = pd.read_csv('tsv/test.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4546e4ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df['url'] = train_df['url'].map(lambda x : x.replace('#','%23'))\n",
        "validation_df['url'] = validation_df['url'].map(lambda x : x.replace('#','%23'))\n",
        "test_df['url'] = test_df['url'].map(lambda x : x.replace('#','%23'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c35571a1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/media/monlamai/FEEE31DEEE31903F/tibetan_voice_v1_wav/wav/SP02-OTR002-01-A-1673.wav'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import os\n",
        "# pdg_train_df['path'] = pdg_train_df['file_name'].apply(lambda x: os.path.join('/media/monlamai/FEEE31DEEE31903F/segments_from_prodigy', x))\n",
        "# pdg_validation_df['path'] = pdg_validation_df['file_name'].apply(lambda x: os.path.join('/media/monlamai/FEEE31DEEE31903F/segments_from_prodigy', x))\n",
        "# pdg_test_df['path'] = pdg_test_df['file_name'].apply(lambda x: os.path.join('/media/monlamai/FEEE31DEEE31903F/segments_from_prodigy', x))\n",
        "\n",
        "# mv_train_df['path'] = mv_train_df['file_name'].apply(lambda x: os.path.join('/home/monlamai/Documents/GitHub/saymore-report-generator/data/segments', x))\n",
        "# mv_validation_df['path'] = mv_validation_df['file_name'].apply(lambda x: os.path.join('/home/monlamai/Documents/GitHub/saymore-report-generator/data/segments', x))\n",
        "# mv_test_df['path'] = mv_test_df['file_name'].apply(lambda x: os.path.join('/home/monlamai/Documents/GitHub/saymore-report-generator/data/segments', x))\n",
        "\n",
        "# v1_train_df['path'] = v1_train_df['file_name'].apply(lambda x: os.path.join('/media/monlamai/FEEE31DEEE31903F/tibetan_voice_v1_wav/wav', x))\n",
        "# v1_validation_df['path'] = v1_validation_df['file_name'].apply(lambda x: os.path.join('/media/monlamai/FEEE31DEEE31903F/tibetan_voice_v1_wav/wav', x))\n",
        "# v1_test_df['path'] = v1_test_df['file_name'].apply(lambda x: os.path.join('/media/monlamai/FEEE31DEEE31903F/tibetan_voice_v1_wav/wav', x))\n",
        "\n",
        "# v1_test_df.path[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "776a51ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 383286, batch_size: 19164, max_batch_i: 20\n",
            "19164\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "total = len(train_df)\n",
        "batch_size = math.floor(total * 25/100)\n",
        "\n",
        "max_batch_i = math.floor(total/batch_size)\n",
        "# batch_df = train_valid_df[batch_i*batch_size:(batch_i+1)*batch_size]\n",
        "print(f'total: {total}, batch_size: {batch_size}, max_batch_i: {max_batch_i}')\n",
        "\n",
        "\n",
        "batch_i = 0 # 0 - 3\n",
        "if batch_i == max_batch_i -1:\n",
        "    batch_df = train_df[batch_i*batch_size:]\n",
        "else :\n",
        "    batch_df = train_df[batch_i*batch_size:(batch_i+1)*batch_size]\n",
        "print(len(batch_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b863cf9",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_df = batch_df[batch_df['file_name'] != 'STT_AB00148_0687_2124469_to_2126579']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
      "metadata": {
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
        "\n",
        "common_voice_train = Dataset.from_pandas(batch_df, split='train')\n",
        "    \n",
        "# uncomment for valid prepare ds only\n",
        "# common_voice_validation = Dataset.from_pandas(validation_df, split='validation')\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = common_voice_train\n",
        "\n",
        "# uncomment for valid prepare ds only\n",
        "# common_voice[\"test\"] = common_voice_validation\n",
        "\n",
        "# print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f",
      "metadata": {
        "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f"
      },
      "source": [
        "Most ASR datasets only provide input audio samples (`audio`) and the\n",
        "corresponding transcribed text (`sentence`). Common Voice contains additional\n",
        "metadata information, such as `accent` and `locale`, which we can disregard for ASR.\n",
        "Keeping the notebook as general as possible, we only consider the input audio and\n",
        "transcribed text for fine-tuning, discarding the additional metadata information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
      "metadata": {
        "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['wylie', 'path'],\n",
            "        num_rows: 19164\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# common_voice = common_voice.remove_columns([\"file_name\", \"uni\", \"dept\", \"__index_level_0__\"])\n",
        "common_voice = common_voice.remove_columns([\"file_name\", \"uni\", \"dept\"])\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605",
      "metadata": {
        "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
      },
      "source": [
        "## Prepare Feature Extractor, Tokenizer and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601c3099-1026-439e-93e2-5635b3ba5a73",
      "metadata": {
        "id": "601c3099-1026-439e-93e2-5635b3ba5a73"
      },
      "source": [
        "The ASR pipeline can be de-composed into three stages:\n",
        "1) A feature extractor which pre-processes the raw audio-inputs\n",
        "2) The model which performs the sequence-to-sequence mapping\n",
        "3) A tokenizer which post-processes the model outputs to text format\n",
        "\n",
        "In 🤗 Transformers, the Whisper model has an associated feature extractor and tokenizer,\n",
        "called [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
        "and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)\n",
        "respectively.\n",
        "\n",
        "We'll go through details for setting-up the feature extractor and tokenizer one-by-one!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560332eb-3558-41a1-b500-e83a9f695f84",
      "metadata": {
        "id": "560332eb-3558-41a1-b500-e83a9f695f84"
      },
      "source": [
        "### Load WhisperFeatureExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365",
      "metadata": {
        "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365"
      },
      "source": [
        "The Whisper feature extractor performs two operations:\n",
        "1. Pads / truncates the audio inputs to 30s: any audio inputs shorter than 30s are padded to 30s with silence (zeros), and those longer that 30s are truncated to 30s\n",
        "2. Converts the audio inputs to _log-Mel spectrogram_ input features, a visual representation of the audio and the form of the input expected by the Whisper model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589d9ec1-d12b-4b64-93f7-04c63997da19",
      "metadata": {
        "id": "589d9ec1-d12b-4b64-93f7-04c63997da19"
      },
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
        "<figcaption align = \"center\"><b>Figure 2:</b> Conversion of sampled audio array to log-Mel spectrogram.\n",
        "Left: sampled 1-dimensional audio signal. Right: corresponding log-Mel spectrogram. Figure source:\n",
        "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Google SpecAugment Blog</a>.\n",
        "</figcaption>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa",
      "metadata": {
        "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa"
      },
      "source": [
        "We'll load the feature extractor from the pre-trained checkpoint with the default values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
      "metadata": {
        "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb",
      "metadata": {
        "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb"
      },
      "source": [
        "### Load WhisperTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc82609-a9fb-447a-a2af-99597c864029",
      "metadata": {
        "id": "2bc82609-a9fb-447a-a2af-99597c864029"
      },
      "source": [
        "The Whisper model outputs a sequence of _token ids_. The tokenizer maps each of these token ids to their corresponding text string. For Hindi, we can load the pre-trained tokenizer and use it for fine-tuning without any further modifications. We simply have to\n",
        "specify the target language and the task. These arguments inform the\n",
        "tokenizer to prefix the language and task tokens to the start of encoded\n",
        "label sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90d056e20b3e4f14ae0199a1a4ab1bb0",
            "d82a88daec0e4f14add691b7b903064c",
            "350acdb0f40e454099fa901e66de55f0",
            "2e6a82a462cc411d90fa1bea4ee60790",
            "c74bfee0198b4817832ea86e8e88d96c",
            "04fb2d81eff646068e10475a08ae42f4"
          ]
        },
        "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
        "outputId": "5c004b44-86e7-4e00-88be-39e0af5eed69"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b3e49f22426410ca5b8ebeda7923350",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bb129813aa74506aece0a0b653ba647",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45873204c11c4e1eae37c0899c362b62",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cb0f17958234d36807e68f4d12ecc28",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)in/added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Tibetan\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b",
      "metadata": {
        "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b"
      },
      "source": [
        "### Combine To Create A WhisperProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d",
      "metadata": {
        "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d"
      },
      "source": [
        "To simplify using the feature extractor and tokenizer, we can _wrap_\n",
        "both into a single `WhisperProcessor` class. This processor object\n",
        "inherits from the `WhisperFeatureExtractor` and `WhisperProcessor`,\n",
        "and can be used on the audio inputs and model predictions as required.\n",
        "In doing so, we only need to keep track of two objects during training:\n",
        "the `processor` and the `model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
      "metadata": {
        "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Tibetan\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
      "metadata": {
        "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f",
      "metadata": {
        "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f"
      },
      "source": [
        "Let's print the first example of the Common Voice dataset to see\n",
        "what form the data is in:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd",
      "metadata": {
        "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd"
      },
      "source": [
        "Since\n",
        "our input audio is sampled at 48kHz, we need to _downsample_ it to\n",
        "16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model.\n",
        "\n",
        "We'll set the audio inputs to the correct sampling rate using dataset's\n",
        "[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column)\n",
        "method. This operation does not change the audio in-place,\n",
        "but rather signals to `datasets` to resample audio samples _on the fly_ the\n",
        "first time that they are loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
      "metadata": {
        "id": "f12e2e57-156f-417b-8cfb-69221cc198e8"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'common_voice' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb Cell 46\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#X63sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Audio\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#X63sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m common_voice \u001b[39m=\u001b[39m common_voice\u001b[39m.\u001b[39mcast_column(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m, Audio(sampling_rate\u001b[39m=\u001b[39m\u001b[39m16000\u001b[39m))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'common_voice' is not defined"
          ]
        }
      ],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"url\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707",
      "metadata": {
        "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707"
      },
      "source": [
        "Re-loading the first audio sample in the Common Voice dataset will resample\n",
        "it to the desired sampling rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "87122d71-289a-466a-afcf-fa354b18946b",
      "metadata": {
        "id": "87122d71-289a-466a-afcf-fa354b18946b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'common_voice' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#X65sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(common_voice[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'common_voice' is not defined"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91edc72d-08f8-4f01-899d-74e65ce441fc",
      "metadata": {
        "id": "91edc72d-08f8-4f01-899d-74e65ce441fc"
      },
      "source": [
        "Now we can write a function to prepare our data ready for the model:\n",
        "1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, 🤗 Datasets performs any necessary resampling operations on the fly.\n",
        "2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
        "3. We encode the transcriptions to label ids through the use of the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6525c478-8962-4394-a1c4-103c54cce170",
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"url\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"wylie\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13",
      "metadata": {
        "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
      },
      "source": [
        "We can apply the data preparation function to all of our training examples using dataset's `.map` method. The argument `num_proc` specifies how many CPU cores to use. Setting `num_proc` > 1 will enable multiprocessing. If the `.map` method hangs with multiprocessing, set `num_proc=1` and process the dataset sequentially."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a025a0",
      "metadata": {},
      "source": [
        "### run batch_size times for train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
      "metadata": {
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3f9a651c429401e98cfdfab3c0b9721",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/19164 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (12845 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "common_voice['train'] = common_voice['train'].map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)\n",
        "common_voice['train'].save_to_disk(f'prepare_dataset_train_batch_{batch_i}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "050685ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_0:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_1:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_10: directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_11: directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_12: directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_13: directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_14: directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_15: directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_2:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_3:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_4:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_5:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_6:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_7:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_8:  directory\n",
            "/media/monlamai/FEEE31DEEE31903F/prepare_dataset_train_batch_9:  directory\n"
          ]
        }
      ],
      "source": [
        "# !file prepare_dataset_train_batch_*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312b7078",
      "metadata": {},
      "source": [
        "### run once for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50dadc28",
      "metadata": {},
      "outputs": [],
      "source": [
        "# common_voice['test'] = common_voice['test'].map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e69fd5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# common_voice['test'].save_to_disk(f'prepare_dataset_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f17e0372",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_from_disk, concatenate_datasets\n",
        "\n",
        "train_batch_0 = load_from_disk('/media/monlamai/HD_volume_1/prepare_dataset_train_batch_0')\n",
        "train_batch_1 = load_from_disk('/media/monlamai/HD_volume_1/prepare_dataset_train_batch_1')\n",
        "train_batch_2 = load_from_disk('/media/monlamai/HD_volume_1/prepare_dataset_train_batch_2')\n",
        "train_batch_3 = load_from_disk('/media/monlamai/HD_volume_1/prepare_dataset_train_batch_3')\n",
        "\n",
        "test =          load_from_disk('/media/monlamai/HD_volume_1/prepare_dataset_test')\n",
        "\n",
        "concatinated_train = concatenate_datasets([train_batch_0, train_batch_1, train_batch_2, train_batch_3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5a10a85c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f39e0839d10941b297c89ebd99c401f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/935 shards):   0%|          | 0/486175 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb428f51948942fe99f7a5310001c60e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/52 shards):   0%|          | 0/27010 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = concatinated_train\n",
        "common_voice[\"test\"]  = test\n",
        "\n",
        "common_voice.save_to_disk('/media/monlamai/SSD/whisper_small_r2_Oct21_prepare_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0a56b7a0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_features', 'labels'],\n",
              "        num_rows: 399041\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_features', 'labels'],\n",
              "        num_rows: 21007\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1b701171",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, load_from_disk\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice = load_from_disk(\"/media/monlamai/SSD/whisper_small_r2_Oct21_prepare_dataset\")\n",
        "# common_voice = load_dataset(\"spsither/whisper_large_r1_ts1693219076.0_prepare_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5d3fe747",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3f1f2cc2d19d490798921409fabd9c67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Pushing dataset shards to the dataset hub:   0%|          | 0/767 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d418e741abe4c7397d03fd52f263b26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a85ea1c4fb049c893f3a5d4112ab698",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a21e12977184d4fb7395b1f91f18d51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Error while uploading 'data/train-00153-of-00767-42498986ece1d23f.parquet' to the Hub.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;31mSSLError\u001b[0m: EOF occurred in violation of protocol (_ssl.c:2426)",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    846\u001b[0m )\n\u001b[1;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    514\u001b[0m     reason \u001b[39m=\u001b[39m error \u001b[39mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[39mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[39mfrom\u001b[39;00m \u001b[39mreason\u001b[39;00m  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mIncremented Retry for (url=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m): \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, url, new_retry)\n",
            "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='s3.us-east-1.amazonaws.com', port=443): Max retries exceeded with url: /lfs.huggingface.co/repos/c8/cd/c8cd37ea0f1dd9f5da6860dcdb749980156492222a950c7a4157416c7cabf193/604f8cb6841a4531b16ad1b0dfc240c1c499a8e0cdbc1a5616890baf2fcde48c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA4N7VTDGO27GPWFUO%2F20230830%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230830T113456Z&X-Amz-Expires=86400&X-Amz-Signature=1d99305313b1d696004bfa6c42b4f17fc2e8fdfe8e56b68f38502e300c06cea9&X-Amz-SignedHeaders=host&partNumber=1&uploadId=gqBubuM_5AO3DWEBu5Wun96a5.xVmsPem7uvcQpLxFo9QzMM6JwZuAQto3kuA6K5_zSonW.XAVyeuGoeD_IjGONZydYE5brk._Zy8pRSr25w5o7.dPFRDCsnZw2yJ8U1&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSSLError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:382\u001b[0m, in \u001b[0;36mupload_lfs_files.<locals>._wrapped_lfs_upload\u001b[0;34m(batch_action)\u001b[0m\n\u001b[1;32m    381\u001b[0m     operation \u001b[39m=\u001b[39m oid2addop[batch_action[\u001b[39m\"\u001b[39m\u001b[39moid\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m--> 382\u001b[0m     lfs_upload(operation\u001b[39m=\u001b[39;49moperation, lfs_batch_action\u001b[39m=\u001b[39;49mbatch_action, token\u001b[39m=\u001b[39;49mtoken)\n\u001b[1;32m    383\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/lfs.py:222\u001b[0m, in \u001b[0;36mlfs_upload\u001b[0;34m(operation, lfs_batch_action, token)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    220\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMalformed response from LFS batch endpoint: `chunk_size` should be an integer. Got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mchunk_size\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[0;32m--> 222\u001b[0m     _upload_multi_part(operation\u001b[39m=\u001b[39;49moperation, header\u001b[39m=\u001b[39;49mheader, chunk_size\u001b[39m=\u001b[39;49mchunk_size, upload_url\u001b[39m=\u001b[39;49mupload_action[\u001b[39m\"\u001b[39;49m\u001b[39mhref\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    223\u001b[0m \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/lfs.py:318\u001b[0m, in \u001b[0;36m_upload_multi_part\u001b[0;34m(operation, header, chunk_size, upload_url)\u001b[0m\n\u001b[1;32m    313\u001b[0m     use_hf_transfer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    315\u001b[0m response_headers \u001b[39m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m     _upload_parts_hf_transfer(operation\u001b[39m=\u001b[39moperation, sorted_parts_urls\u001b[39m=\u001b[39msorted_parts_urls, chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m    317\u001b[0m     \u001b[39mif\u001b[39;00m use_hf_transfer\n\u001b[0;32m--> 318\u001b[0m     \u001b[39melse\u001b[39;00m _upload_parts_iteratively(operation\u001b[39m=\u001b[39;49moperation, sorted_parts_urls\u001b[39m=\u001b[39;49msorted_parts_urls, chunk_size\u001b[39m=\u001b[39;49mchunk_size)\n\u001b[1;32m    319\u001b[0m )\n\u001b[1;32m    321\u001b[0m \u001b[39m# 3. Send completion request\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/lfs.py:374\u001b[0m, in \u001b[0;36m_upload_parts_iteratively\u001b[0;34m(operation, sorted_parts_urls, chunk_size)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39mwith\u001b[39;00m SliceFileObj(\n\u001b[1;32m    370\u001b[0m     fileobj,\n\u001b[1;32m    371\u001b[0m     seek_from\u001b[39m=\u001b[39mchunk_size \u001b[39m*\u001b[39m part_idx,\n\u001b[1;32m    372\u001b[0m     read_limit\u001b[39m=\u001b[39mchunk_size,\n\u001b[1;32m    373\u001b[0m ) \u001b[39mas\u001b[39;00m fileobj_slice:\n\u001b[0;32m--> 374\u001b[0m     part_upload_res \u001b[39m=\u001b[39m http_backoff(\u001b[39m\"\u001b[39;49m\u001b[39mPUT\u001b[39;49m\u001b[39m\"\u001b[39;49m, part_upload_url, data\u001b[39m=\u001b[39;49mfileobj_slice)\n\u001b[1;32m    375\u001b[0m     hf_raise_for_status(part_upload_res)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:258\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:63\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     64\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mRequestException \u001b[39mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/requests/adapters.py:517\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, _SSLError):\n\u001b[1;32m    516\u001b[0m     \u001b[39m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m--> 517\u001b[0m     \u001b[39mraise\u001b[39;00m SSLError(e, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(e, request\u001b[39m=\u001b[39mrequest)\n",
            "\u001b[0;31mSSLError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='s3.us-east-1.amazonaws.com', port=443): Max retries exceeded with url: /lfs.huggingface.co/repos/c8/cd/c8cd37ea0f1dd9f5da6860dcdb749980156492222a950c7a4157416c7cabf193/604f8cb6841a4531b16ad1b0dfc240c1c499a8e0cdbc1a5616890baf2fcde48c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA4N7VTDGO27GPWFUO%2F20230830%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230830T113456Z&X-Amz-Expires=86400&X-Amz-Signature=1d99305313b1d696004bfa6c42b4f17fc2e8fdfe8e56b68f38502e300c06cea9&X-Amz-SignedHeaders=host&partNumber=1&uploadId=gqBubuM_5AO3DWEBu5Wun96a5.xVmsPem7uvcQpLxFo9QzMM6JwZuAQto3kuA6K5_zSonW.XAVyeuGoeD_IjGONZydYE5brk._Zy8pRSr25w5o7.dPFRDCsnZw2yJ8U1&x-id=UploadPart (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:2426)')))\"), '(Request ID: 4a2285db-7baa-4956-9eb0-379cf8e423c2)')",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m common_voice\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39mspsither/whisper_large_r1_ts1693219076.0_prepare_dataset\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/datasets/dataset_dict.py:1641\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, config_name, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1639\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPushing split \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m to the Hub.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1640\u001b[0m \u001b[39m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[0;32m-> 1641\u001b[0m repo_id, split, uploaded_size, dataset_nbytes, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[split]\u001b[39m.\u001b[39;49m_push_parquet_shards_to_hub(\n\u001b[1;32m   1642\u001b[0m     repo_id,\n\u001b[1;32m   1643\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1644\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m   1645\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[1;32m   1646\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1647\u001b[0m     branch\u001b[39m=\u001b[39;49mbranch,\n\u001b[1;32m   1648\u001b[0m     max_shard_size\u001b[39m=\u001b[39;49mmax_shard_size,\n\u001b[1;32m   1649\u001b[0m     num_shards\u001b[39m=\u001b[39;49mnum_shards\u001b[39m.\u001b[39;49mget(split),\n\u001b[1;32m   1650\u001b[0m     embed_external_files\u001b[39m=\u001b[39;49membed_external_files,\n\u001b[1;32m   1651\u001b[0m )\n\u001b[1;32m   1652\u001b[0m total_uploaded_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m uploaded_size\n\u001b[1;32m   1653\u001b[0m total_dataset_nbytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dataset_nbytes\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/datasets/arrow_dataset.py:5307\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, data_dir, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5305\u001b[0m         shard\u001b[39m.\u001b[39mto_parquet(buffer)\n\u001b[1;32m   5306\u001b[0m         uploaded_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39mtell()\n\u001b[0;32m-> 5307\u001b[0m         _retry(\n\u001b[1;32m   5308\u001b[0m             api\u001b[39m.\u001b[39;49mupload_file,\n\u001b[1;32m   5309\u001b[0m             func_kwargs\u001b[39m=\u001b[39;49m{\n\u001b[1;32m   5310\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpath_or_fileobj\u001b[39;49m\u001b[39m\"\u001b[39;49m: buffer\u001b[39m.\u001b[39;49mgetvalue(),\n\u001b[1;32m   5311\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpath_in_repo\u001b[39;49m\u001b[39m\"\u001b[39;49m: shard_path_in_repo,\n\u001b[1;32m   5312\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mrepo_id\u001b[39;49m\u001b[39m\"\u001b[39;49m: repo_id,\n\u001b[1;32m   5313\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtoken\u001b[39;49m\u001b[39m\"\u001b[39;49m: token,\n\u001b[1;32m   5314\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mrepo_type\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   5315\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mrevision\u001b[39;49m\u001b[39m\"\u001b[39;49m: branch,\n\u001b[1;32m   5316\u001b[0m             },\n\u001b[1;32m   5317\u001b[0m             exceptions\u001b[39m=\u001b[39;49mHTTPError,\n\u001b[1;32m   5318\u001b[0m             status_codes\u001b[39m=\u001b[39;49m[\u001b[39m504\u001b[39;49m],\n\u001b[1;32m   5319\u001b[0m             base_wait_time\u001b[39m=\u001b[39;49m\u001b[39m2.0\u001b[39;49m,\n\u001b[1;32m   5320\u001b[0m             max_retries\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m   5321\u001b[0m             max_wait_time\u001b[39m=\u001b[39;49m\u001b[39m20.0\u001b[39;49m,\n\u001b[1;32m   5322\u001b[0m         )\n\u001b[1;32m   5323\u001b[0m     shards_path_in_repo\u001b[39m.\u001b[39mappend(shard_path_in_repo)\n\u001b[1;32m   5325\u001b[0m \u001b[39m# Cleanup to remove unused files\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/datasets/utils/file_utils.py:290\u001b[0m, in \u001b[0;36m_retry\u001b[0;34m(func, func_args, func_kwargs, exceptions, status_codes, max_retries, base_wait_time, max_wait_time)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m    291\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    292\u001b[0m         \u001b[39mif\u001b[39;00m retry \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_retries \u001b[39mor\u001b[39;00m (status_codes \u001b[39mand\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m status_codes):\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/hf_api.py:844\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_as_future(fn, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    843\u001b[0m \u001b[39m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 844\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3236\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   3228\u001b[0m commit_message \u001b[39m=\u001b[39m (\n\u001b[1;32m   3229\u001b[0m     commit_message \u001b[39mif\u001b[39;00m commit_message \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUpload \u001b[39m\u001b[39m{\u001b[39;00mpath_in_repo\u001b[39m}\u001b[39;00m\u001b[39m with huggingface_hub\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3230\u001b[0m )\n\u001b[1;32m   3231\u001b[0m operation \u001b[39m=\u001b[39m CommitOperationAdd(\n\u001b[1;32m   3232\u001b[0m     path_or_fileobj\u001b[39m=\u001b[39mpath_or_fileobj,\n\u001b[1;32m   3233\u001b[0m     path_in_repo\u001b[39m=\u001b[39mpath_in_repo,\n\u001b[1;32m   3234\u001b[0m )\n\u001b[0;32m-> 3236\u001b[0m commit_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_commit(\n\u001b[1;32m   3237\u001b[0m     repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   3238\u001b[0m     repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   3239\u001b[0m     operations\u001b[39m=\u001b[39;49m[operation],\n\u001b[1;32m   3240\u001b[0m     commit_message\u001b[39m=\u001b[39;49mcommit_message,\n\u001b[1;32m   3241\u001b[0m     commit_description\u001b[39m=\u001b[39;49mcommit_description,\n\u001b[1;32m   3242\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   3243\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   3244\u001b[0m     create_pr\u001b[39m=\u001b[39;49mcreate_pr,\n\u001b[1;32m   3245\u001b[0m     parent_commit\u001b[39m=\u001b[39;49mparent_commit,\n\u001b[1;32m   3246\u001b[0m )\n\u001b[1;32m   3248\u001b[0m \u001b[39mif\u001b[39;00m commit_info\u001b[39m.\u001b[39mpr_url \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3249\u001b[0m     revision \u001b[39m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[39m.\u001b[39mpr_url), safe\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/hf_api.py:844\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_as_future(fn, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    843\u001b[0m \u001b[39m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 844\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2710\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   2701\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   2702\u001b[0m files_to_copy \u001b[39m=\u001b[39m fetch_lfs_files_to_copy(\n\u001b[1;32m   2703\u001b[0m     copies\u001b[39m=\u001b[39mcopies,\n\u001b[1;32m   2704\u001b[0m     repo_type\u001b[39m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2708\u001b[0m     endpoint\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint,\n\u001b[1;32m   2709\u001b[0m )\n\u001b[0;32m-> 2710\u001b[0m upload_lfs_files(\n\u001b[1;32m   2711\u001b[0m     additions\u001b[39m=\u001b[39;49m[addition \u001b[39mfor\u001b[39;49;00m addition \u001b[39min\u001b[39;49;00m additions \u001b[39mif\u001b[39;49;00m upload_modes[addition\u001b[39m.\u001b[39;49mpath_in_repo] \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mlfs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2712\u001b[0m     repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   2713\u001b[0m     repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   2714\u001b[0m     token\u001b[39m=\u001b[39;49mtoken \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m   2715\u001b[0m     endpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mendpoint,\n\u001b[1;32m   2716\u001b[0m     num_threads\u001b[39m=\u001b[39;49mnum_threads,\n\u001b[1;32m   2717\u001b[0m )\n\u001b[1;32m   2718\u001b[0m commit_payload \u001b[39m=\u001b[39m prepare_commit_payload(\n\u001b[1;32m   2719\u001b[0m     operations\u001b[39m=\u001b[39moperations,\n\u001b[1;32m   2720\u001b[0m     upload_modes\u001b[39m=\u001b[39mupload_modes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2724\u001b[0m     parent_commit\u001b[39m=\u001b[39mparent_commit,\n\u001b[1;32m   2725\u001b[0m )\n\u001b[1;32m   2726\u001b[0m commit_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendpoint\u001b[39m}\u001b[39;00m\u001b[39m/api/\u001b[39m\u001b[39m{\u001b[39;00mrepo_type\u001b[39m}\u001b[39;00m\u001b[39ms/\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m/commit/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:392\u001b[0m, in \u001b[0;36mupload_lfs_files\u001b[0;34m(additions, repo_type, repo_id, token, endpoint, num_threads)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(filtered_actions) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    391\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mUploading 1 LFS file to the Hub\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 392\u001b[0m     _wrapped_lfs_upload(filtered_actions[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m    393\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    394\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    395\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUploading \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(filtered_actions)\u001b[39m}\u001b[39;00m\u001b[39m LFS files to the Hub using up to \u001b[39m\u001b[39m{\u001b[39;00mnum_threads\u001b[39m}\u001b[39;00m\u001b[39m threads concurrently\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/huggingface_hub/_commit_api.py:384\u001b[0m, in \u001b[0;36mupload_lfs_files.<locals>._wrapped_lfs_upload\u001b[0;34m(batch_action)\u001b[0m\n\u001b[1;32m    382\u001b[0m     lfs_upload(operation\u001b[39m=\u001b[39moperation, lfs_batch_action\u001b[39m=\u001b[39mbatch_action, token\u001b[39m=\u001b[39mtoken)\n\u001b[1;32m    383\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m--> 384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError while uploading \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m.\u001b[39mpath_in_repo\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m to the Hub.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error while uploading 'data/train-00153-of-00767-42498986ece1d23f.parquet' to the Hub."
          ]
        }
      ],
      "source": [
        "# common_voice.push_to_hub(\"spsither/whisper_large_r1_ts1693219076.0_prepare_dataset\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
      "metadata": {
        "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
      },
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7",
      "metadata": {
        "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7"
      },
      "source": [
        "Now that we've prepared our data, we're ready to dive into the training pipeline.\n",
        "The [🤗 Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)\n",
        "will do much of the heavy lifting for us. All we have to do is:\n",
        "\n",
        "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
        "\n",
        "- Evaluation metrics: during evaluation, we want to evaluate the model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric. We need to define a `compute_metrics` function that handles this computation.\n",
        "\n",
        "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
        "\n",
        "- Define the training configuration: this will be used by the 🤗 Trainer to define the training schedule.\n",
        "\n",
        "Once we've fine-tuned the model, we will evaluate it on the test data to verify that we have correctly trained it\n",
        "to transcribe speech in Hindi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
      "metadata": {
        "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
      },
      "source": [
        "### Define a Data Collator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04def221-0637-4a69-b242-d3f0c1d0ee78",
      "metadata": {
        "id": "04def221-0637-4a69-b242-d3f0c1d0ee78"
      },
      "source": [
        "The data collator for a sequence-to-sequence speech model is unique in the sense that it\n",
        "treats the `input_features` and `labels` independently: the  `input_features` must be\n",
        "handled by the feature extractor and the `labels` by the tokenizer.\n",
        "\n",
        "The `input_features` are already padded to 30s and converted to a log-Mel spectrogram\n",
        "of fixed dimension by action of the feature extractor, so all we have to do is convert the `input_features`\n",
        "to batched PyTorch tensors. We do this using the feature extractor's `.pad` method with `return_tensors=pt`.\n",
        "\n",
        "The `labels` on the other hand are un-padded. We first pad the sequences\n",
        "to the maximum length in the batch using the tokenizer's `.pad` method. The padding tokens\n",
        "are then replaced by `-100` so that these tokens are **not** taken into account when\n",
        "computing the loss. We then cut the BOS token from the start of the label sequence as we\n",
        "append it later during training.\n",
        "\n",
        "We can leverage the `WhisperProcessor` we defined earlier to perform both the\n",
        "feature extractor and the tokenizer operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
      "metadata": {
        "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
      },
      "source": [
        "Let's initialise the data collator we've just defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
      "metadata": {
        "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
      "metadata": {
        "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fee1a7-a44c-461e-b047-c3917221572e",
      "metadata": {
        "id": "66fee1a7-a44c-461e-b047-c3917221572e"
      },
      "source": [
        "We'll use the word error rate (WER) metric, the 'de-facto' metric for assessing\n",
        "ASR systems. For more information, refer to the WER [docs](https://huggingface.co/metrics/wer). We'll load the WER metric from 🤗 Evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b22b4011-f31f-4b57-b684-c52332f92890",
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508",
      "metadata": {
        "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508"
      },
      "source": [
        "We then simply have to define a function that takes our model\n",
        "predictions and returns the WER metric. This function, called\n",
        "`compute_metrics`, first replaces `-100` with the `pad_token_id`\n",
        "in the `label_ids` (undoing the step we applied in the\n",
        "data collator to ignore padded tokens correctly in the loss).\n",
        "It then decodes the predicted and label ids to strings. Finally,\n",
        "it computes the WER between the predictions and reference labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52",
      "metadata": {
        "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * min(metric.compute(predictions=pred_str, references=label_str), 1)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7183a9",
      "metadata": {},
      "source": [
        "### Load a Pre-Trained Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437a97fa-4864-476b-8abc-f28b8166cfa5",
      "metadata": {
        "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
      },
      "source": [
        "Now let's load the pre-trained Whisper `small` checkpoint. Again, this\n",
        "is trivial through use of 🤗 Transformers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
      "metadata": {
        "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15ead5f-2277-4a39-937b-585c2497b2df",
      "metadata": {
        "id": "a15ead5f-2277-4a39-937b-585c2497b2df"
      },
      "source": [
        "Override generation arguments - no tokens are forced as decoder outputs (see [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), no tokens are suppressed during generation (see [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9c86de5e",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.generation_config.no_repeat_ngram_size = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
      "metadata": {
        "id": "62038ba3-88ed-4fce-84db-338f50dcd04f"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06",
      "metadata": {
        "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
      },
      "source": [
        "### Define the Training Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21af1e9-0188-4134-ac82-defc7bdcc436",
      "metadata": {
        "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
      },
      "source": [
        "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-r2\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=32,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=500,\n",
        "    # max_steps=4000,\n",
        "    num_train_epochs = 10,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=5000,\n",
        "    eval_steps=5000,\n",
        "    logging_steps=100,\n",
        "    save_total_limit=10,\n",
        "    report_to=[\"wandb\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a944d8-3112-4552-82a0-be25988b3857",
      "metadata": {
        "id": "b3a944d8-3112-4552-82a0-be25988b3857"
      },
      "source": [
        "**Note**: if one does not want to upload the model checkpoints to the Hub,\n",
        "set `push_to_hub=False`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac29114-d226-4f54-97cf-8718c9f94e1e",
      "metadata": {
        "id": "bac29114-d226-4f54-97cf-8718c9f94e1e"
      },
      "source": [
        "We can forward the training arguments to the 🤗 Trainer along with our model,\n",
        "dataset, data collator and `compute_metrics` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d546d7fe-0543-479a-b708-2ebabec19493",
      "metadata": {
        "id": "d546d7fe-0543-479a-b708-2ebabec19493"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb Cell 86\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Seq2SeqTrainer\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer \u001b[39m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mcommon_voice[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mcommon_voice[\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mprocessor\u001b[39m.\u001b[39;49mfeature_extractor,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/monlamai/Documents/GitHub/stt-whisper/fine_tune_whisper.ipynb#Y150sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:56\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     43\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     44\u001b[0m     model: Union[\u001b[39m\"\u001b[39m\u001b[39mPreTrainedModel\u001b[39m\u001b[39m\"\u001b[39m, nn\u001b[39m.\u001b[39mModule] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], torch\u001b[39m.\u001b[39mTensor]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     55\u001b[0m ):\n\u001b[0;32m---> 56\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     57\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     58\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m     59\u001b[0m         data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[1;32m     60\u001b[0m         train_dataset\u001b[39m=\u001b[39;49mtrain_dataset,\n\u001b[1;32m     61\u001b[0m         eval_dataset\u001b[39m=\u001b[39;49meval_dataset,\n\u001b[1;32m     62\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     63\u001b[0m         model_init\u001b[39m=\u001b[39;49mmodel_init,\n\u001b[1;32m     64\u001b[0m         compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics,\n\u001b[1;32m     65\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m     66\u001b[0m         optimizers\u001b[39m=\u001b[39;49moptimizers,\n\u001b[1;32m     67\u001b[0m         preprocess_logits_for_metrics\u001b[39m=\u001b[39;49mpreprocess_logits_for_metrics,\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     \u001b[39m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[39m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgeneration_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:504\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[39m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplace_model_on_device\n\u001b[1;32m    502\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mquantization_method\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m QuantizationMethod\u001b[39m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    503\u001b[0m ):\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_move_model_to_device(model, args\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    506\u001b[0m \u001b[39m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_model_parallel:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:728\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_move_model_to_device\u001b[39m(\u001b[39mself\u001b[39m, model, device):\n\u001b[0;32m--> 728\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    729\u001b[0m     \u001b[39m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    730\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mparallel_mode \u001b[39m==\u001b[39m ParallelMode\u001b[39m.\u001b[39mTPU \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39mtie_weights\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/modeling_utils.py:2050\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2045\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2046\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2047\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2048\u001b[0m     )\n\u001b[1;32m   2049\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2050\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUDA-capable device(s) is/are busy or unavailable\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uOrRhDGtN5S4",
      "metadata": {
        "id": "uOrRhDGtN5S4"
      },
      "source": [
        "We'll save the processor object once before starting training. Since the processor is not trainable, it won't change over the course of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "-2zQwMfEOBJq",
      "metadata": {
        "id": "-2zQwMfEOBJq"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b88c539",
      "metadata": {},
      "outputs": [],
      "source": [
        "max_label_length = model.config.max_length\n",
        "\n",
        "def filter_labels(labels_length):\n",
        "    \"\"\"Filter label sequences longer than max length (448)\"\"\"\n",
        "    return len(labels_length) < max_label_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ef6dab",
      "metadata": {},
      "outputs": [],
      "source": [
        "common_voice['train'] = common_voice['train'].filter(filter_labels, input_columns=[\"labels\"])\n",
        "common_voice['test'] = common_voice['test'].filter(filter_labels, input_columns=[\"labels\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f404cf9-4345-468c-8196-4bd101d9bd51",
      "metadata": {
        "id": "7f404cf9-4345-468c-8196-4bd101d9bd51"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112",
      "metadata": {
        "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112"
      },
      "source": [
        "Training will take approximately 5-10 hours depending on your GPU or the one\n",
        "allocated to this Google Colab. If using this Google Colab directly to\n",
        "fine-tune a Whisper model, you should make sure that training isn't\n",
        "interrupted due to inactivity. A simple workaround to prevent this is\n",
        "to paste the following code into the console of this tab (_right mouse click_\n",
        "-> _inspect_ -> _Console tab_ -> _insert code_)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890a63ed-e87b-4e53-a35a-6ec1eca560af",
      "metadata": {
        "id": "890a63ed-e87b-4e53-a35a-6ec1eca560af"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton, 60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a55168b-2f46-4678-afa0-ff22257ec06d",
      "metadata": {
        "id": "5a55168b-2f46-4678-afa0-ff22257ec06d"
      },
      "source": [
        "The peak GPU memory for the given training configuration is approximately 15.8GB.\n",
        "Depending on the GPU allocated to the Google Colab, it is possible that you will encounter a CUDA `\"out-of-memory\"` error when you launch training.\n",
        "In this case, you can reduce the `per_device_train_batch_size` incrementally by factors of 2\n",
        "and employ [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps)\n",
        "to compensate.\n",
        "\n",
        "To launch training, simply execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "848dc7bc8bfb46e583019bdce8107ee0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.68 GiB (GPU 0; 23.65 GiB total capacity; 18.99 GiB already allocated; 270.12 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[39m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1547\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1548\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1549\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1550\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1551\u001b[0m     )\n\u001b[1;32m   1552\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2681\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2682\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2684\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:2707\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2705\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2706\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2707\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2708\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2709\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2710\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/accelerate/utils/operations.py:581\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/accelerate/utils/operations.py:569\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/accelerate/utils/operations.py:581\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m model_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/accelerate/utils/operations.py:569\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m convert_to_fp32(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/amp/autocast_mode.py:14\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_autocast\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     13\u001b[0m     \u001b[39mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1486\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoder_inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1482\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1483\u001b[0m             labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1484\u001b[0m         )\n\u001b[0;32m-> 1486\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1487\u001b[0m     input_features,\n\u001b[1;32m   1488\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1489\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1490\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1491\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1492\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1493\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1494\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1495\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1496\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1497\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1498\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1499\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1500\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1501\u001b[0m )\n\u001b[1;32m   1502\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj_out(outputs[\u001b[39m0\u001b[39m])\n\u001b[1;32m   1504\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:1346\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     input_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mask_input_features(input_features, attention_mask\u001b[39m=\u001b[39mattention_mask)\n\u001b[0;32m-> 1346\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1347\u001b[0m         input_features,\n\u001b[1;32m   1348\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1349\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1350\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1351\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1352\u001b[0m     )\n\u001b[1;32m   1353\u001b[0m \u001b[39m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1354\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:935\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39minputs, output_attentions)\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m custom_forward\n\u001b[0;32m--> 935\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mcheckpoint\u001b[39m.\u001b[39;49mcheckpoint(\n\u001b[1;32m    936\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    937\u001b[0m         hidden_states,\n\u001b[1;32m    938\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    939\u001b[0m         (head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    940\u001b[0m     )\n\u001b[1;32m    941\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     layer_outputs \u001b[39m=\u001b[39m encoder_layer(\n\u001b[1;32m    943\u001b[0m         hidden_states,\n\u001b[1;32m    944\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    945\u001b[0m         layer_head_mask\u001b[39m=\u001b[39m(head_mask[idx] \u001b[39mif\u001b[39;00m head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    946\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[1;32m    947\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:249\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, *args, **kwargs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnexpected keyword arguments: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(arg \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m kwargs))\n\u001b[1;32m    248\u001b[0m \u001b[39mif\u001b[39;00m use_reentrant:\n\u001b[0;32m--> 249\u001b[0m     \u001b[39mreturn\u001b[39;00m CheckpointFunction\u001b[39m.\u001b[39;49mapply(function, preserve, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    250\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    251\u001b[0m     \u001b[39mreturn\u001b[39;00m _checkpoint_without_reentrant(\n\u001b[1;32m    252\u001b[0m         function,\n\u001b[1;32m    253\u001b[0m         preserve,\n\u001b[1;32m    254\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[1;32m    255\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    256\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:107\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    104\u001b[0m ctx\u001b[39m.\u001b[39msave_for_backward(\u001b[39m*\u001b[39mtensor_inputs)\n\u001b[1;32m    106\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 107\u001b[0m     outputs \u001b[39m=\u001b[39m run_function(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:931\u001b[0m, in \u001b[0;36mWhisperEncoder.forward.<locals>.create_custom_forward.<locals>.custom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_forward\u001b[39m(\u001b[39m*\u001b[39minputs):\n\u001b[0;32m--> 931\u001b[0m     \u001b[39mreturn\u001b[39;00m module(\u001b[39m*\u001b[39;49minputs, output_attentions)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:505\u001b[0m, in \u001b[0;36mWhisperEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    503\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    504\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m--> 505\u001b[0m hidden_states, attn_weights, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    506\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    507\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    508\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    509\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    510\u001b[0m )\n\u001b[1;32m    511\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    512\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/models/whisper/modeling_whisper.py:424\u001b[0m, in \u001b[0;36mWhisperAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    421\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mview(bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len) \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    422\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[0;32m--> 424\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attn_weights, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    426\u001b[0m \u001b[39mif\u001b[39;00m layer_head_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     \u001b[39mif\u001b[39;00m layer_head_mask\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,):\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/nn/functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1841\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1842\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1843\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[1;32m   1844\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1845\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.68 GiB (GPU 0; 23.65 GiB total capacity; 18.99 GiB already allocated; 270.12 MiB free; 21.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3",
      "metadata": {
        "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3"
      },
      "source": [
        "Our best WER is 32.0% - not bad for 8h of training data! We can submit our checkpoint to the [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench) on push by setting the appropriate key-word arguments (kwargs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c704f91e-241b-48c9-b8e0-f0da396a9663",
      "metadata": {
        "id": "c704f91e-241b-48c9-b8e0-f0da396a9663"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: hi, split: test\",\n",
        "    \"language\": \"hi\",\n",
        "    \"model_name\": \"Whisper large Hi - Sanchit Gandhi\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "    \"tags\": \"hf-asr-leaderboard\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090d676a-f944-4297-a938-a40eda0b2b68",
      "metadata": {
        "id": "090d676a-f944-4297-a938-a40eda0b2b68"
      },
      "source": [
        "The training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` command and save the preprocessor object we created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d7030622-caf7-4039-939b-6195cdaa2585",
      "metadata": {
        "id": "d7030622-caf7-4039-939b-6195cdaa2585"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/spsither/whisper-small-hi/tree/main/'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d4360d-5721-426e-b6ac-178f833fedeb",
      "metadata": {
        "id": "34d4360d-5721-426e-b6ac-178f833fedeb"
      },
      "source": [
        "## Building a Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9",
      "metadata": {
        "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9"
      },
      "source": [
        "Now that we've fine-tuned our model we can build a demo to show\n",
        "off its ASR capabilities! We'll make use of 🤗 Transformers\n",
        "`pipeline`, which will take care of the entire ASR pipeline,\n",
        "right from pre-processing the audio inputs to decoding the\n",
        "model predictions.\n",
        "\n",
        "Running the example below will generate a Gradio demo where we\n",
        "can record speech through the microphone of our computer and input it to\n",
        "our fine-tuned Whisper model to transcribe the corresponding text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa",
      "metadata": {
        "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c3841399e614510a12eaf81ecfcc69a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/monlamai/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "pipe = pipeline(model=\"spsither/whisper-small-r2\")  # change to \"your-username/the-name-you-picked\"\n",
        "\n",
        "def transcribe(audio):\n",
        "    text = pipe(audio)[\"text\"]\n",
        "    return text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Large Tibetan\",\n",
        "    description=\"Realtime demo for Tibetan speech recognition using a fine-tuned Whisper medium model.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba",
      "metadata": {
        "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba"
      },
      "source": [
        "## Closing Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f737783-2870-4e35-aa11-86a42d7d997a",
      "metadata": {
        "id": "7f737783-2870-4e35-aa11-86a42d7d997a"
      },
      "source": [
        "In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR\n",
        "using 🤗 Datasets, Transformers and the Hugging Face Hub. For more details on the Whisper model, the Common Voice dataset and the theory behind fine-tuning, refere to the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). If you're interested in fine-tuning other\n",
        "Transformers models, both for English and multilingual ASR, be sure to check out the\n",
        "examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
