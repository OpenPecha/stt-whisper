{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6",
      "metadata": {
        "id": "75b58048-7d14-4fc6-8085-1fc08c81b4a6"
      },
      "source": [
        "# Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfa8ad5-4cdc-4512-9058-836cbbf65e1a",
      "metadata": {
        "id": "fbfa8ad5-4cdc-4512-9058-836cbbf65e1a"
      },
      "source": [
        "In this Colab, we present a step-by-step guide on how to fine-tune Whisper\n",
        "for any multilingual ASR dataset using Hugging Face 🤗 Transformers. This is a\n",
        "more \"hands-on\" version of the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper).\n",
        "For a more in-depth explanation of Whisper, the Common Voice dataset and the theory behind fine-tuning, the reader is advised to refer to the blog post."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe0d503-ae4e-4aa7-9af4-dbcba52db41e",
      "metadata": {
        "id": "afe0d503-ae4e-4aa7-9af4-dbcba52db41e"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae91ed4-9c3e-4ade-938e-f4c2dcfbfdc0",
      "metadata": {
        "id": "9ae91ed4-9c3e-4ade-938e-f4c2dcfbfdc0"
      },
      "source": [
        "Whisper is a pre-trained model for automatic speech recognition (ASR)\n",
        "published in [September 2022](https://openai.com/blog/whisper/) by the authors\n",
        "Alec Radford et al. from OpenAI. Unlike many of its predecessors, such as\n",
        "[Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), which are pre-trained\n",
        "on un-labelled audio data, Whisper is pre-trained on a vast quantity of\n",
        "**labelled** audio-transcription data, 680,000 hours to be precise.\n",
        "This is an order of magnitude more data than the un-labelled audio data used\n",
        "to train Wav2Vec 2.0 (60,000 hours). What is more, 117,000 hours of this\n",
        "pre-training data is multilingual ASR data. This results in checkpoints\n",
        "that can be applied to over 96 languages, many of which are considered\n",
        "_low-resource_.\n",
        "\n",
        "When scaled to 680,000 hours of labelled pre-training data, Whisper models\n",
        "demonstrate a strong ability to generalise to many datasets and domains.\n",
        "The pre-trained checkpoints achieve competitive results to state-of-the-art\n",
        "ASR systems, with near 3% word error rate (WER) on the test-clean subset of\n",
        "LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._\n",
        "Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).\n",
        "The extensive multilingual ASR knowledge acquired by Whisper during pre-training\n",
        "can be leveraged for other low-resource languages; through fine-tuning, the\n",
        "pre-trained checkpoints can be adapted for specific datasets and languages\n",
        "to further improve upon these results. We'll show just how Whisper can be fine-tuned\n",
        "for low-resource languages in this Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21b6316e-8a55-4549-a154-66d3da2ab74a",
      "metadata": {
        "id": "21b6316e-8a55-4549-a154-66d3da2ab74a"
      },
      "source": [
        "The Whisper checkpoints come in five configurations of varying model sizes.\n",
        "The smallest four are trained on either English-only or multilingual data.\n",
        "The largest checkpoint is multilingual only. All nine of the pre-trained checkpoints\n",
        "are available on the [Hugging Face Hub](https://huggingface.co/models?search=openai/whisper). The\n",
        "checkpoints are summarised in the following table with links to the models on the Hub:\n",
        "\n",
        "| Size   | Layers | Width | Heads | Parameters | English-only                                         | Multilingual                                      |\n",
        "|--------|--------|-------|-------|------------|------------------------------------------------------|---------------------------------------------------|\n",
        "| tiny   | 4      | 384   | 6     | 39 M       | [✓](https://huggingface.co/openai/whisper-tiny.en)   | [✓](https://huggingface.co/openai/whisper-tiny.)  |\n",
        "| base   | 6      | 512   | 8     | 74 M       | [✓](https://huggingface.co/openai/whisper-base.en)   | [✓](https://huggingface.co/openai/whisper-base)   |\n",
        "| small  | 12     | 768   | 12    | 244 M      | [✓](https://huggingface.co/openai/whisper-small.en)  | [✓](https://huggingface.co/openai/whisper-small)  |\n",
        "| medium | 24     | 1024  | 16    | 769 M      | [✓](https://huggingface.co/openai/whisper-medium.en) | [✓](https://huggingface.co/openai/whisper-medium) |\n",
        "| large  | 32     | 1280  | 20    | 1550 M     | x                                                    | [✓](https://huggingface.co/openai/whisper-large)  |\n",
        "\n",
        "For demonstration purposes, we'll fine-tune the multilingual version of the\n",
        "[`\"small\"`](https://huggingface.co/openai/whisper-small) checkpoint with 244M params (~= 1GB).\n",
        "As for our data, we'll train and evaluate our system on a low-resource language\n",
        "taken from the [Common Voice](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0)\n",
        "dataset. We'll show that with as little as 8 hours of fine-tuning data, we can achieve\n",
        "strong performance in this language."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a680dfc-cbba-4f6c-8a1f-e1a5ff3f123a",
      "metadata": {
        "id": "3a680dfc-cbba-4f6c-8a1f-e1a5ff3f123a"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "\n",
        "\\\\({}^1\\\\) The name Whisper follows from the acronym “WSPSR”, which stands for “Web-scale Supervised Pre-training for Speech Recognition”."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55fb8d21-df06-472a-99dd-b59567be6dad",
      "metadata": {
        "id": "55fb8d21-df06-472a-99dd-b59567be6dad"
      },
      "source": [
        "## Prepare Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "844a4861-929c-4762-b29b-80b1e95aba4b",
      "metadata": {
        "id": "844a4861-929c-4762-b29b-80b1e95aba4b"
      },
      "source": [
        "First of all, let's try to secure a decent GPU for our Colab! Unfortunately, it's becoming much harder to get access to a good GPU with the free version of Google Colab. However, with Google Colab Pro one should have no issues in being allocated a V100 or P100 GPU.\n",
        "\n",
        "To get a GPU, click _Runtime_ -> _Change runtime type_, then change _Hardware accelerator_ from _None_ to _GPU_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a",
      "metadata": {
        "id": "9abea5d7-9d54-434b-a6bd-399d1b3c6c1a"
      },
      "source": [
        "We can verify that we've been assigned a GPU and view its specifications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "95048026-a3b7-43f0-a274-1bad65e407b4",
      "metadata": {
        "id": "95048026-a3b7-43f0-a274-1bad65e407b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Aug 21 10:17:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0  On |                  Off |\n",
            "|  0%   40C    P8    35W / 450W |    530MiB / 24564MiB |     12%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1446      G   /usr/lib/xorg/Xorg                155MiB |\n",
            "|    0   N/A  N/A      2118      G   /usr/bin/gnome-shell               83MiB |\n",
            "|    0   N/A  N/A      4527      G   ...587173396459472978,262144       87MiB |\n",
            "|    0   N/A  N/A      5582      G   ...veSuggestionsOnlyOnDemand       52MiB |\n",
            "|    0   N/A  N/A     69088      G   ...RendererForSitePerProcess      147MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d",
      "metadata": {
        "id": "1d85d613-1c7e-46ac-9134-660bbe7ebc9d"
      },
      "source": [
        "We'll employ several popular Python packages to fine-tune the Whisper model.\n",
        "We'll use `datasets` to download and prepare our training data and\n",
        "`transformers` to load and train our Whisper model. We'll also require\n",
        "the `soundfile` package to pre-process audio files, `evaluate` and `jiwer` to\n",
        "assess the performance of our model. Finally, we'll\n",
        "use `gradio` to build a flashy demo of our fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e68ea9f8-9b61-414e-8885-3033b67c2850",
      "metadata": {
        "id": "e68ea9f8-9b61-414e-8885-3033b67c2850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: 2.6.1 not found\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-nsw8vjpj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-nsw8vjpj\n",
            "  Resolved https://github.com/huggingface/transformers to commit 1982dd3b15867c46e1c20645901b0de469fd935f\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting tqdm>=4.27\n",
            "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Using cached regex-2023.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from transformers==4.32.0.dev0) (23.1)\n",
            "Collecting safetensors>=0.3.1\n",
            "  Using cached safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1\n",
            "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "Collecting numpy>=1.17\n",
            "  Using cached numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Using cached tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "Collecting typing-extensions>=3.7.4.3\n",
            "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting fsspec\n",
            "  Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
            "Collecting urllib3<3,>=1.21.1\n",
            "  Using cached urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for transformers: filename=transformers-4.32.0.dev0-py3-none-any.whl size=7522589 sha256=8e418bd81b4601ba121bfc19fda754287268ce2631c9a39fc281fe510f49dfda\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xh8h6ivp/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: tokenizers, safetensors, urllib3, typing-extensions, tqdm, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, transformers\n",
            "Successfully installed certifi-2023.7.22 charset-normalizer-3.2.0 filelock-3.12.2 fsspec-2023.6.0 huggingface-hub-0.16.4 idna-3.4 numpy-1.25.2 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 safetensors-0.3.2 tokenizers-0.13.3 tqdm-4.66.1 transformers-4.32.0.dev0 typing-extensions-4.7.1 urllib3-2.0.4\n",
            "Collecting librosa\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in ./.env/lib/python3.10/site-packages (from librosa) (1.25.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in ./.env/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
            "Collecting numba>=0.51.0\n",
            "  Using cached numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "Collecting msgpack>=1.0\n",
            "  Using cached msgpack-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (316 kB)\n",
            "Collecting pooch>=1.0\n",
            "  Downloading pooch-1.7.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m299.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.2.0\n",
            "  Downloading scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting soxr>=0.3.2\n",
            "  Using cached soxr-0.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting soundfile>=0.12.1\n",
            "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_31_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in ./.env/lib/python3.10/site-packages (from librosa) (4.7.1)\n",
            "Collecting joblib>=0.14\n",
            "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "Collecting audioread>=2.1.9\n",
            "  Using cached audioread-3.0.0.tar.gz (377 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting scikit-learn>=0.20.0\n",
            "  Using cached scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Collecting lazy-loader>=0.1\n",
            "  Using cached lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
            "Collecting llvmlite<0.41,>=0.40.0dev0\n",
            "  Using cached llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
            "Collecting numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3\n",
            "  Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from pooch>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in ./.env/lib/python3.10/site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting cffi>=1.0\n",
            "  Using cached cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
            "Using legacy 'setup.py install' for audioread, since package 'wheel' is not installed.\n",
            "Installing collected packages: msgpack, threadpoolctl, pycparser, numpy, llvmlite, lazy-loader, joblib, audioread, soxr, scipy, pooch, numba, cffi, soundfile, scikit-learn, librosa\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Running setup.py install for audioread ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed audioread-3.0.0 cffi-1.15.1 joblib-1.3.2 lazy-loader-0.3 librosa-0.10.1 llvmlite-0.40.1 msgpack-1.0.5 numba-0.57.1 numpy-1.24.4 pooch-1.7.0 pycparser-2.21 scikit-learn-1.3.0 scipy-1.11.2 soundfile-0.12.1 soxr-0.3.6 threadpoolctl-3.2.0\n",
            "zsh:1: 0.30 not found\n",
            "Collecting jiwer\n",
            "  Using cached jiwer-3.0.2-py3-none-any.whl (21 kB)\n",
            "Collecting click<9.0.0,>=8.1.3\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz==2.13.7\n",
            "  Using cached rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "Installing collected packages: rapidfuzz, click, jiwer\n",
            "Successfully installed click-8.1.7 jiwer-3.0.2 rapidfuzz-2.13.7\n",
            "Collecting gradio\n",
            "  Using cached gradio-3.40.1-py3-none-any.whl (20.0 MB)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Using cached mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in ./.env/lib/python3.10/site-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from gradio) (23.1)\n",
            "Collecting markupsafe~=2.0\n",
            "  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting jinja2<4.0\n",
            "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "Collecting aiohttp~=3.0\n",
            "  Using cached aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Collecting pillow<11.0,>=8.0\n",
            "  Using cached Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.env/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
            "Collecting httpx\n",
            "  Using cached httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "Collecting ffmpy\n",
            "  Using cached ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting semantic-version~=2.0\n",
            "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting matplotlib~=3.0\n",
            "  Using cached matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "Collecting python-multipart\n",
            "  Using cached python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in ./.env/lib/python3.10/site-packages (from gradio) (4.7.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4\n",
            "  Downloading pydantic-2.2.1-py3-none-any.whl (373 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.4/373.4 KB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting pandas<3.0,>=1.0\n",
            "  Using cached pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "Collecting pydub\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting importlib-resources<7.0,>=1.3\n",
            "  Using cached importlib_resources-6.0.1-py3-none-any.whl (34 kB)\n",
            "Collecting gradio-client>=0.4.0\n",
            "  Using cached gradio_client-0.4.0-py3-none-any.whl (297 kB)\n",
            "Collecting altair<6.0,>=4.2.0\n",
            "  Using cached altair-5.0.1-py3-none-any.whl (471 kB)\n",
            "Collecting uvicorn>=0.14.0\n",
            "  Using cached uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "Collecting fastapi\n",
            "  Using cached fastapi-0.101.1-py3-none-any.whl (65 kB)\n",
            "Collecting orjson~=3.0\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests~=2.0 in ./.env/lib/python3.10/site-packages (from gradio) (2.31.0)\n",
            "Collecting websockets<12.0,>=10.0\n",
            "  Using cached websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "Requirement already satisfied: numpy~=1.0 in ./.env/lib/python3.10/site-packages (from gradio) (1.24.4)\n",
            "Collecting aiofiles<24.0,>=22.0\n",
            "  Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Using cached yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Using cached frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.env/lib/python3.10/site-packages (from aiohttp~=3.0->gradio) (3.2.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Using cached multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "Collecting toolz\n",
            "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
            "Collecting jsonschema>=3.0\n",
            "  Using cached jsonschema-4.19.0-py3-none-any.whl (83 kB)\n",
            "Requirement already satisfied: fsspec in ./.env/lib/python3.10/site-packages (from gradio-client>=0.4.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (4.66.1)\n",
            "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.14.0->gradio) (3.12.2)\n",
            "Collecting mdurl~=0.1\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Using cached linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Using cached contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.env/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Collecting pyparsing<3.1,>=2.3.1\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.42.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Using cached mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "  Using cached mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "  Using cached mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "  Using cached mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "  Using cached mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "  Using cached mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Using cached mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Using cached mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "  Using cached mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting matplotlib~=3.0\n",
            "  Using cached matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "INFO: pip is looking at multiple versions of markupsafe to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting markupsafe~=2.0\n",
            "  Using cached MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "INFO: pip is looking at multiple versions of markdown-it-py[linkify] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting markdown-it-py[linkify]>=2.0.0\n",
            "  Using cached markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "Collecting pytz>=2020.1\n",
            "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "Collecting tzdata>=2022.1\n",
            "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "Collecting annotated-types>=0.4.0\n",
            "  Using cached annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
            "Collecting pydantic-core==2.6.1\n",
            "  Downloading pydantic_core-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests~=2.0->gradio) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests~=2.0->gradio) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests~=2.0->gradio) (2.0.4)\n",
            "Collecting h11>=0.8\n",
            "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "Requirement already satisfied: click>=7.0 in ./.env/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (8.1.7)\n",
            "Collecting starlette<0.28.0,>=0.27.0\n",
            "  Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "Collecting httpcore<0.18.0,>=0.15.0\n",
            "  Using cached httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "Collecting sniffio\n",
            "  Using cached sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting anyio<5.0,>=3.0\n",
            "  Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6\n",
            "  Using cached jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)\n",
            "Collecting rpds-py>=0.7.1\n",
            "  Using cached rpds_py-0.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Collecting referencing>=0.28.4\n",
            "  Using cached referencing-0.30.2-py3-none-any.whl (25 kB)\n",
            "Collecting uc-micro-py\n",
            "  Using cached uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Collecting exceptiongroup\n",
            "  Using cached exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Using legacy 'setup.py install' for ffmpy, since package 'wheel' is not installed.\n",
            "Installing collected packages: pytz, pydub, ffmpy, websockets, uc-micro-py, tzdata, toolz, sniffio, semantic-version, rpds-py, python-multipart, pyparsing, pydantic-core, pillow, orjson, multidict, mdurl, markupsafe, kiwisolver, importlib-resources, h11, frozenlist, fonttools, exceptiongroup, cycler, contourpy, attrs, async-timeout, annotated-types, aiofiles, yarl, uvicorn, referencing, pydantic, pandas, matplotlib, markdown-it-py, linkify-it-py, jinja2, anyio, aiosignal, starlette, mdit-py-plugins, jsonschema-specifications, httpcore, aiohttp, jsonschema, httpx, fastapi, gradio-client, altair, gradio\n",
            "  Running setup.py install for ffmpy ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed aiofiles-23.2.1 aiohttp-3.8.5 aiosignal-1.3.1 altair-5.0.1 annotated-types-0.5.0 anyio-3.7.1 async-timeout-4.0.3 attrs-23.1.0 contourpy-1.1.0 cycler-0.11.0 exceptiongroup-1.1.3 fastapi-0.101.1 ffmpy-0.3.1 fonttools-4.42.1 frozenlist-1.4.0 gradio-3.40.1 gradio-client-0.4.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 importlib-resources-6.0.1 jinja2-3.1.2 jsonschema-4.19.0 jsonschema-specifications-2023.7.1 kiwisolver-1.4.4 linkify-it-py-2.0.2 markdown-it-py-2.2.0 markupsafe-2.1.3 matplotlib-3.7.2 mdit-py-plugins-0.3.3 mdurl-0.1.2 multidict-6.0.4 orjson-3.9.5 pandas-2.0.3 pillow-10.0.0 pydantic-2.2.1 pydantic-core-2.6.1 pydub-0.25.1 pyparsing-3.0.9 python-multipart-0.0.6 pytz-2023.3 referencing-0.30.2 rpds-py-0.9.2 semantic-version-2.10.0 sniffio-1.3.0 starlette-0.27.0 toolz-0.12.0 tzdata-2023.3 uc-micro-py-1.0.2 uvicorn-0.23.2 websockets-11.0.3 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203",
      "metadata": {
        "id": "1f60d173-8de1-4ed7-bc9a-d281cf237203"
      },
      "source": [
        "We strongly advise you to upload model checkpoints directly the [Hugging Face Hub](https://huggingface.co/)\n",
        "whilst training. The Hub provides:\n",
        "- Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
        "- Tensorboard logs: track important metrics over the course of training.\n",
        "- Model cards: document what a model does and its intended use cases.\n",
        "- Community: an easy way to share and collaborate with the community!\n",
        "\n",
        "Linking the notebook to the Hub is straightforward - it simply requires entering your\n",
        "Hub authentication token when prompted. Find your Hub authentication token [here](https://huggingface.co/settings/tokens):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0ad9c753",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/huggingface_hub\n",
            "  Cloning https://github.com/huggingface/huggingface_hub to /tmp/pip-req-build-2uwjeu6j\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/huggingface_hub /tmp/pip-req-build-2uwjeu6j\n",
            "  Resolved https://github.com/huggingface/huggingface_hub to commit b9901bf71839850c10ea42162c69aecabc96a46b\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: requests in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (3.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (4.7.1)\n",
            "Requirement already satisfied: fsspec in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from huggingface-hub==0.17.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests->huggingface-hub==0.17.0.dev0) (3.2.0)\n",
            "Collecting ipywidgets\n",
            "  Using cached ipywidgets-8.1.0-py3-none-any.whl (139 kB)\n",
            "Collecting widgetsnbextension~=4.0.7\n",
            "  Using cached widgetsnbextension-4.0.8-py3-none-any.whl (2.3 MB)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in ./.env/lib/python3.10/site-packages (from ipywidgets) (5.9.0)\n",
            "Collecting jupyterlab-widgets~=3.0.7\n",
            "  Using cached jupyterlab_widgets-3.0.8-py3-none-any.whl (214 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in ./.env/lib/python3.10/site-packages (from ipywidgets) (8.14.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in ./.env/lib/python3.10/site-packages (from ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: jedi>=0.16 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.0)\n",
            "Requirement already satisfied: matplotlib-inline in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.39)\n",
            "Requirement already satisfied: stack-data in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
            "Requirement already satisfied: pexpect>4.3 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: pygments>=2.4.0 in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: pickleshare in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: decorator in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
            "Requirement already satisfied: backcall in ./.env/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.env/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in ./.env/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in ./.env/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in ./.env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: pure-eval in ./.env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: executing>=1.2.0 in ./.env/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: six in ./.env/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
            "Installing collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
            "Successfully installed ipywidgets-8.1.0 jupyterlab-widgets-3.0.8 widgetsnbextension-4.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/huggingface/huggingface_hub\n",
        "!pip install wandb -qU\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b045a39e-2a3e-4153-bdb5-281500bcd348",
      "metadata": {
        "id": "b045a39e-2a3e-4153-bdb5-281500bcd348"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e3f7fc934bb42439445405d10627f7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "# 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\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "63fa31ee",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgaychetenzin\u001b[0m (\u001b[33mmonlam-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "#879f84539f271b958e4068c102c776ed213da057\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cca143d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
            "               [--paths] [--json] [--debug]\n",
            "               [subcommand]\n",
            "\n",
            "Jupyter: Interactive Computing\n",
            "\n",
            "positional arguments:\n",
            "  subcommand     the subcommand to launch\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --version      show the versions of core jupyter packages and exit\n",
            "  --config-dir   show Jupyter config dir\n",
            "  --data-dir     show Jupyter data dir\n",
            "  --runtime-dir  show Jupyter runtime dir\n",
            "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
            "                 format.\n",
            "  --json         output paths as machine-readable json\n",
            "  --debug        output debug information about paths\n",
            "\n",
            "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
            "\n",
            "Jupyter command `jupyter-notebook` not found.\n",
            "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
            "               [--paths] [--json] [--debug]\n",
            "               [subcommand]\n",
            "\n",
            "Jupyter: Interactive Computing\n",
            "\n",
            "positional arguments:\n",
            "  subcommand     the subcommand to launch\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --version      show the versions of core jupyter packages and exit\n",
            "  --config-dir   show Jupyter config dir\n",
            "  --data-dir     show Jupyter data dir\n",
            "  --runtime-dir  show Jupyter runtime dir\n",
            "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
            "                 format.\n",
            "  --json         output paths as machine-readable json\n",
            "  --debug        output debug information about paths\n",
            "\n",
            "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
            "\n",
            "Jupyter command `jupyter-notebook` not found.\n",
            "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
            "               [--paths] [--json] [--debug]\n",
            "               [subcommand]\n",
            "\n",
            "Jupyter: Interactive Computing\n",
            "\n",
            "positional arguments:\n",
            "  subcommand     the subcommand to launch\n",
            "\n",
            "options:\n",
            "  -h, --help     show this help message and exit\n",
            "  --version      show the versions of core jupyter packages and exit\n",
            "  --config-dir   show Jupyter config dir\n",
            "  --data-dir     show Jupyter data dir\n",
            "  --runtime-dir  show Jupyter runtime dir\n",
            "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
            "                 format.\n",
            "  --json         output paths as machine-readable json\n",
            "  --debug        output debug information about paths\n",
            "\n",
            "Available subcommands: kernel kernelspec migrate run troubleshoot\n",
            "\n",
            "Jupyter command `jupyter-notebook` not found.\n"
          ]
        }
      ],
      "source": [
        "#https://towardsdatascience.com/leveraging-the-power-of-jupyter-notebooks-26b4b8d7c622\n",
        "!jupyter notebook --generate-config\n",
        "!jupyter notebook --NotebookApp.max_buffer_size=64000000000\n",
        "!jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0",
      "metadata": {
        "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674429c5-0ab4-4adf-975b-621bb69eca38",
      "metadata": {
        "id": "674429c5-0ab4-4adf-975b-621bb69eca38"
      },
      "source": [
        "Using 🤗 Datasets, downloading and preparing data is extremely simple.\n",
        "We can download and prepare the Common Voice splits in just one line of code.\n",
        "\n",
        "First, ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally.\n",
        "\n",
        "Since Hindi is very low-resource, we'll combine the `train` and `validation`\n",
        "splits to give approximately 8 hours of training data. We'll use the 4 hours\n",
        "of `test` data as our held-out test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "261d0cc7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "Collecting pyarrow>=8.0.0\n",
            "  Using cached pyarrow-12.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
            "Collecting multiprocess\n",
            "  Using cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "Collecting xxhash\n",
            "  Using cached xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in ./.env/lib/python3.10/site-packages (from datasets) (0.17.0.dev0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in ./.env/lib/python3.10/site-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from datasets) (23.1)\n",
            "Collecting dill<0.3.8,>=0.3.0\n",
            "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in ./.env/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: aiohttp in ./.env/lib/python3.10/site-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in ./.env/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 pyarrow-12.0.1 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip  install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "51c02e2c",
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir tsv mv_tsv v1_tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ed601493",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 88.7M  100 88.7M    0     0  10.9M      0  0:00:08  0:00:08 --:--:-- 11.5M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5063k  100 5063k    0     0  10.0M      0 --:--:-- --:--:-- --:--:-- 10.0M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 5025k  100 5025k    0     0  8641k      0 --:--:-- --:--:-- --:--:-- 8634k\n"
          ]
        }
      ],
      "source": [
        "!curl https://d38pmlk0v88drf.cloudfront.net/tsv/train.tsv --output tsv/train.tsv\n",
        "!curl https://d38pmlk0v88drf.cloudfront.net/tsv/test.tsv --output tsv/test.tsv\n",
        "!curl https://d38pmlk0v88drf.cloudfront.net/tsv/validation.tsv --output tsv/validation.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "79ffc8d7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 35.3M  100 35.3M    0     0  8294k      0  0:00:04  0:00:04 --:--:-- 8829k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2004k  100 2004k    0     0  5645k      0 --:--:-- --:--:-- --:--:-- 5646k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1995k  100 1995k    0     0  6129k      0 --:--:-- --:--:-- --:--:-- 6121k\n"
          ]
        }
      ],
      "source": [
        "!curl https://d38pmlk0v88drf.cloudfront.net/mv_tsv/train.tsv --output      mv_tsv/train.tsv\n",
        "!curl https://d38pmlk0v88drf.cloudfront.net/mv_tsv/test.tsv --output       mv_tsv/test.tsv\n",
        "!curl https://d38pmlk0v88drf.cloudfront.net/mv_tsv/validation.tsv --output mv_tsv/validation.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "69721a73",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1638k  100 1638k    0     0  3994k      0 --:--:-- --:--:-- --:--:-- 3987k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 94021  100 94021    0     0   401k      0 --:--:-- --:--:-- --:--:--  402k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 97437  100 97437    0     0   426k      0 --:--:-- --:--:-- --:--:--  428k\n"
          ]
        }
      ],
      "source": [
        "!curl https://d38pmlk0v88drf.cloudfront.net/v1_tsv/train.tsv --output      v1_tsv/train.tsv\n",
        "!curl https://d38pmlk0v88drf.cloudfront.net/v1_tsv/test.tsv --output       v1_tsv/test.tsv\n",
        "!curl https://d38pmlk0v88drf.cloudfront.net/v1_tsv/validation.tsv --output v1_tsv/validation.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "42177717",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.concat([pd.read_csv('tsv/train.tsv', sep='\\t'), pd.read_csv('mv_tsv/train.tsv', sep='\\t'), pd.read_csv('v1_tsv/train.tsv', sep='\\t')])\n",
        "validation_df = pd.concat([pd.read_csv('tsv/validation.tsv', sep='\\t'), pd.read_csv('mv_tsv/validation.tsv', sep='\\t'), pd.read_csv('v1_tsv/validation.tsv', sep='\\t')])\n",
        "test_df = pd.concat([pd.read_csv('tsv/test.tsv', sep='\\t'), pd.read_csv('mv_tsv/test.tsv', sep='\\t'), pd.read_csv('v1_tsv/test.tsv', sep='\\t')])\n",
        "\n",
        "# train_df = pd.read_csv('v1_tsv/train.tsv', sep='\\t')\n",
        "# validation_df = pd.read_csv('v1_tsv/validation.tsv', sep='\\t')\n",
        "# test_df = pd.read_csv('v1_tsv/test.tsv', sep='\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fd145d16",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df['url'] = train_df['url'].map(lambda x : x.replace('#','%23'))\n",
        "validation_df['url'] = validation_df['url'].map(lambda x : x.replace('#','%23'))\n",
        "test_df['url'] = test_df['url'].map(lambda x : x.replace('#','%23'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a43fef2c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>uni</th>\n",
              "      <th>wylie</th>\n",
              "      <th>url</th>\n",
              "      <th>dept</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>STT_CS-L-2014_2014-M_M-D_D-B04_B02-3_3-R_R-01_...</td>\n",
              "      <td>ཨེ། ང་ཚོ་ཁྱིམ་ཚང་ཨ་ཅག་དེ་ཚོ་ལ་རཱ། སྐད་ཆ་མང་པོ་...</td>\n",
              "      <td>e/_nga tsho khyim tshang a cag de tsho la rA/_...</td>\n",
              "      <td>https://d38pmlk0v88drf.cloudfront.net/wav/STT_...</td>\n",
              "      <td>STT_CS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>STT_CS-D-2011-P-D-B04-4-R-01a_0079_811257_to_8...</td>\n",
              "      <td>མི་གཅིག་ཡོད་རེད་དཱ། བུ་མོ་གཅིག་ཀ་ཅི་ཡོད་རཱ། པད...</td>\n",
              "      <td>mi gcig yod red dA/_bu mo gcig ka ci yod rA/_p...</td>\n",
              "      <td>https://d38pmlk0v88drf.cloudfront.net/wav/STT_...</td>\n",
              "      <td>STT_CS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>STT_CS-D-2013-M-D-B04-2-R-01b_0009_51122_to_55...</td>\n",
              "      <td>ཨ་ནས་འདུག་བ། མོ་ཡིག་གཟུགས་ཡག་པོ་བྱས་ནས་འདུག་བ།...</td>\n",
              "      <td>a nas 'dug ba/_mo yig gzugs yag po byas nas 'd...</td>\n",
              "      <td>https://d38pmlk0v88drf.cloudfront.net/wav/STT_...</td>\n",
              "      <td>STT_CS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>STT_CS-L-2019_2019-M_P-D_D-B02_B02-0_0-Y_Y-01a...</td>\n",
              "      <td>འོ་འདི། ཨོམ་མ་ཎི་པདྨེ་ཧཱུམ་ཧྲི། ཨོམ།</td>\n",
              "      <td>'o 'di/_om ma Ni pad+me hUm hri/_om/</td>\n",
              "      <td>https://d38pmlk0v88drf.cloudfront.net/wav/STT_...</td>\n",
              "      <td>STT_CS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>STT_CS-D-2011-M-D-B02-5-R-02_0024_211350_to_22...</td>\n",
              "      <td>ཅ་ལག་འདྲ་པོ་འཚོལ་བསྡད་དུས། ཨེ་ནས། རྗེས་ལ་པད་ཤག...</td>\n",
              "      <td>ca lag 'dra po 'tshol bsdad dus/_e nas/_rjes l...</td>\n",
              "      <td>https://d38pmlk0v88drf.cloudfront.net/wav/STT_...</td>\n",
              "      <td>STT_CS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           file_name  \\\n",
              "0  STT_CS-L-2014_2014-M_M-D_D-B04_B02-3_3-R_R-01_...   \n",
              "1  STT_CS-D-2011-P-D-B04-4-R-01a_0079_811257_to_8...   \n",
              "2  STT_CS-D-2013-M-D-B04-2-R-01b_0009_51122_to_55...   \n",
              "3  STT_CS-L-2019_2019-M_P-D_D-B02_B02-0_0-Y_Y-01a...   \n",
              "4  STT_CS-D-2011-M-D-B02-5-R-02_0024_211350_to_22...   \n",
              "\n",
              "                                                 uni  \\\n",
              "0  ཨེ། ང་ཚོ་ཁྱིམ་ཚང་ཨ་ཅག་དེ་ཚོ་ལ་རཱ། སྐད་ཆ་མང་པོ་...   \n",
              "1  མི་གཅིག་ཡོད་རེད་དཱ། བུ་མོ་གཅིག་ཀ་ཅི་ཡོད་རཱ། པད...   \n",
              "2  ཨ་ནས་འདུག་བ། མོ་ཡིག་གཟུགས་ཡག་པོ་བྱས་ནས་འདུག་བ།...   \n",
              "3               འོ་འདི། ཨོམ་མ་ཎི་པདྨེ་ཧཱུམ་ཧྲི། ཨོམ།   \n",
              "4  ཅ་ལག་འདྲ་པོ་འཚོལ་བསྡད་དུས། ཨེ་ནས། རྗེས་ལ་པད་ཤག...   \n",
              "\n",
              "                                               wylie  \\\n",
              "0  e/_nga tsho khyim tshang a cag de tsho la rA/_...   \n",
              "1  mi gcig yod red dA/_bu mo gcig ka ci yod rA/_p...   \n",
              "2  a nas 'dug ba/_mo yig gzugs yag po byas nas 'd...   \n",
              "3               'o 'di/_om ma Ni pad+me hUm hri/_om/   \n",
              "4  ca lag 'dra po 'tshol bsdad dus/_e nas/_rjes l...   \n",
              "\n",
              "                                                 url    dept  \n",
              "0  https://d38pmlk0v88drf.cloudfront.net/wav/STT_...  STT_CS  \n",
              "1  https://d38pmlk0v88drf.cloudfront.net/wav/STT_...  STT_CS  \n",
              "2  https://d38pmlk0v88drf.cloudfront.net/wav/STT_...  STT_CS  \n",
              "3  https://d38pmlk0v88drf.cloudfront.net/wav/STT_...  STT_CS  \n",
              "4  https://d38pmlk0v88drf.cloudfront.net/wav/STT_...  STT_CS  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d81f4478",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "358740 19936\n"
          ]
        }
      ],
      "source": [
        "print(len(train_df), len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a2787582-554f-44ce-9f38-4180a5ed6b44",
      "metadata": {
        "id": "a2787582-554f-44ce-9f38-4180a5ed6b44"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
        "\n",
        "common_voice_train = Dataset.from_pandas(train_df, split='train')\n",
        "common_voice_test = Dataset.from_pandas(test_df, split='test')\n",
        "common_voice_validation = Dataset.from_pandas(validation_df, split='validation')\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = concatenate_datasets([common_voice_train, common_voice_validation])\n",
        "common_voice[\"test\"] = common_voice_test\n",
        "\n",
        "# common_voice[\"train\"] = load_dataset(\"openpecha/tibetan_voice\", \"lhasa-wylie\", split=\"train+validation\", use_auth_token=True)\n",
        "# common_voice[\"test\"] = load_dataset(\"openpecha/tibetan_voice\", \"lhasa-wylie\", split=\"test\", use_auth_token=True)\n",
        "\n",
        "# print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f",
      "metadata": {
        "id": "d5c7c3d6-7197-41e7-a088-49b753c1681f"
      },
      "source": [
        "Most ASR datasets only provide input audio samples (`audio`) and the\n",
        "corresponding transcribed text (`sentence`). Common Voice contains additional\n",
        "metadata information, such as `accent` and `locale`, which we can disregard for ASR.\n",
        "Keeping the notebook as general as possible, we only consider the input audio and\n",
        "transcribed text for fine-tuning, discarding the additional metadata information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f6935dca",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'file_name': 'STT_CS-D-2011-P-D-B04-4-R-01a_0012_165619_to_167999.wav',\n",
              " 'uni': 'འགྲིག་སོང་། ང་ཁ་ལག་འགྲིག་སོང་ལབ་རེད་དཱ་རཱ། རྗེས་ལ་',\n",
              " 'wylie': \"'grig song /_nga kha lag 'grig song lab red dA rA/_rjes la \",\n",
              " 'url': 'https://d38pmlk0v88drf.cloudfront.net/wav/STT_CS-D-2011-P-D-B04-4-R-01a_0012_165619_to_167999.wav',\n",
              " 'dept': 'STT_CS',\n",
              " '__index_level_0__': 0}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice['test'][0]\n",
        "# test_df.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce",
      "metadata": {
        "id": "20ba635d-518c-47ac-97ee-3cad25f1e0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['wylie', 'url'],\n",
            "        num_rows: 378667\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['wylie', 'url'],\n",
            "        num_rows: 19936\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "common_voice = common_voice.remove_columns([\"file_name\", \"uni\", \"dept\", \"__index_level_0__\"])\n",
        "# common_voice = common_voice.remove_columns([\"file_name\", \"uni\", \"dept\"])\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605",
      "metadata": {
        "id": "2d63b2d2-f68a-4d74-b7f1-5127f6d16605"
      },
      "source": [
        "## Prepare Feature Extractor, Tokenizer and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601c3099-1026-439e-93e2-5635b3ba5a73",
      "metadata": {
        "id": "601c3099-1026-439e-93e2-5635b3ba5a73"
      },
      "source": [
        "The ASR pipeline can be de-composed into three stages:\n",
        "1) A feature extractor which pre-processes the raw audio-inputs\n",
        "2) The model which performs the sequence-to-sequence mapping\n",
        "3) A tokenizer which post-processes the model outputs to text format\n",
        "\n",
        "In 🤗 Transformers, the Whisper model has an associated feature extractor and tokenizer,\n",
        "called [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)\n",
        "and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer)\n",
        "respectively.\n",
        "\n",
        "We'll go through details for setting-up the feature extractor and tokenizer one-by-one!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "560332eb-3558-41a1-b500-e83a9f695f84",
      "metadata": {
        "id": "560332eb-3558-41a1-b500-e83a9f695f84"
      },
      "source": [
        "### Load WhisperFeatureExtractor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365",
      "metadata": {
        "id": "32ec8068-0bd7-412d-b662-0edb9d1e7365"
      },
      "source": [
        "The Whisper feature extractor performs two operations:\n",
        "1. Pads / truncates the audio inputs to 30s: any audio inputs shorter than 30s are padded to 30s with silence (zeros), and those longer that 30s are truncated to 30s\n",
        "2. Converts the audio inputs to _log-Mel spectrogram_ input features, a visual representation of the audio and the form of the input expected by the Whisper model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589d9ec1-d12b-4b64-93f7-04c63997da19",
      "metadata": {
        "id": "589d9ec1-d12b-4b64-93f7-04c63997da19"
      },
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/spectrogram.jpg\" alt=\"Trulli\" style=\"width:100%\">\n",
        "<figcaption align = \"center\"><b>Figure 2:</b> Conversion of sampled audio array to log-Mel spectrogram.\n",
        "Left: sampled 1-dimensional audio signal. Right: corresponding log-Mel spectrogram. Figure source:\n",
        "<a href=\"https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html\">Google SpecAugment Blog</a>.\n",
        "</figcaption>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa",
      "metadata": {
        "id": "b2ef54d5-b946-4c1d-9fdc-adc5d01b46aa"
      },
      "source": [
        "We'll load the feature extractor from the pre-trained checkpoint with the default values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5",
      "metadata": {
        "id": "bc77d7bb-f9e2-47f5-b663-30f7a4321ce5"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb",
      "metadata": {
        "id": "93748af7-b917-4ecf-a0c8-7d89077ff9cb"
      },
      "source": [
        "### Load WhisperTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc82609-a9fb-447a-a2af-99597c864029",
      "metadata": {
        "id": "2bc82609-a9fb-447a-a2af-99597c864029"
      },
      "source": [
        "The Whisper model outputs a sequence of _token ids_. The tokenizer maps each of these token ids to their corresponding text string. For Hindi, we can load the pre-trained tokenizer and use it for fine-tuning without any further modifications. We simply have to\n",
        "specify the target language and the task. These arguments inform the\n",
        "tokenizer to prefix the language and task tokens to the start of encoded\n",
        "label sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90d056e20b3e4f14ae0199a1a4ab1bb0",
            "d82a88daec0e4f14add691b7b903064c",
            "350acdb0f40e454099fa901e66de55f0",
            "2e6a82a462cc411d90fa1bea4ee60790",
            "c74bfee0198b4817832ea86e8e88d96c",
            "04fb2d81eff646068e10475a08ae42f4"
          ]
        },
        "id": "c7b07f9b-ae0e-4f89-98f0-0c50d432eab6",
        "outputId": "5c004b44-86e7-4e00-88be-39e0af5eed69"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-large\", language=\"Tibetan\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b",
      "metadata": {
        "id": "d2ef23f3-f4a8-483a-a2dc-080a7496cb1b"
      },
      "source": [
        "### Combine To Create A WhisperProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d",
      "metadata": {
        "id": "5ff67654-5a29-4bb8-a69d-0228946c6f8d"
      },
      "source": [
        "To simplify using the feature extractor and tokenizer, we can _wrap_\n",
        "both into a single `WhisperProcessor` class. This processor object\n",
        "inherits from the `WhisperFeatureExtractor` and `WhisperProcessor`,\n",
        "and can be used on the audio inputs and model predictions as required.\n",
        "In doing so, we only need to keep track of two objects during training:\n",
        "the `processor` and the `model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6",
      "metadata": {
        "id": "77d9f0c5-8607-4642-a8ac-c3ab2e223ea6"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\", language=\"Tibetan\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c",
      "metadata": {
        "id": "381acd09-0b0f-4d04-9eb3-f028ac0e5f2c"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f",
      "metadata": {
        "id": "9649bf01-2e8a-45e5-8fca-441c13637b8f"
      },
      "source": [
        "Let's print the first example of the Common Voice dataset to see\n",
        "what form the data is in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255",
      "metadata": {
        "id": "6e6b0ec5-0c94-4e2c-ae24-c791be1b2255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'wylie': \"e/_nga tsho khyim tshang a cag de tsho la rA/_skad cha mang po bshad kyi rA/_a cag de tsho tshig dzor bo dzor bo btang gi 'dug dA rA/_'ong /\", 'url': 'https://d38pmlk0v88drf.cloudfront.net/wav/STT_CS-L-2014_2014-M_M-D_D-B04_B02-3_3-R_R-01_0101_857207_to_865881.wav'}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd",
      "metadata": {
        "id": "5a679f05-063d-41b3-9b58-4fc9c6ccf4fd"
      },
      "source": [
        "Since\n",
        "our input audio is sampled at 48kHz, we need to _downsample_ it to\n",
        "16kHz prior to passing it to the Whisper feature extractor, 16kHz being the sampling rate expected by the Whisper model.\n",
        "\n",
        "We'll set the audio inputs to the correct sampling rate using dataset's\n",
        "[`cast_column`](https://huggingface.co/docs/datasets/package_reference/main_classes.html?highlight=cast_column#datasets.DatasetDict.cast_column)\n",
        "method. This operation does not change the audio in-place,\n",
        "but rather signals to `datasets` to resample audio samples _on the fly_ the\n",
        "first time that they are loaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f12e2e57-156f-417b-8cfb-69221cc198e8",
      "metadata": {
        "id": "f12e2e57-156f-417b-8cfb-69221cc198e8"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"url\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707",
      "metadata": {
        "id": "00382a3e-abec-4cdd-a54c-d1aaa3ea4707"
      },
      "source": [
        "Re-loading the first audio sample in the Common Voice dataset will resample\n",
        "it to the desired sampling rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "87122d71-289a-466a-afcf-fa354b18946b",
      "metadata": {
        "id": "87122d71-289a-466a-afcf-fa354b18946b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'wylie': 'mi gcig yod red dA/_bu mo gcig ka ci yod rA/_pad shag rku ma brgyabs kyi red dA rA/_bu mo des pho lis-i la lab kyi red dA rA/_pho lis-i gis rA/_ga re lab dgos red/_bu mo de la bu de la bzung gi red dA rA/', 'url': {'path': 'https://d38pmlk0v88drf.cloudfront.net/wav/STT_CS-D-2011-P-D-B04-4-R-01a_0079_811257_to_820386.wav', 'array': array([-0.00299072, -0.00558472, -0.00680542, ..., -0.00363159,\n",
            "       -0.00473022, -0.00396729]), 'sampling_rate': 16000}}\n"
          ]
        }
      ],
      "source": [
        "print(common_voice[\"train\"][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91edc72d-08f8-4f01-899d-74e65ce441fc",
      "metadata": {
        "id": "91edc72d-08f8-4f01-899d-74e65ce441fc"
      },
      "source": [
        "Now we can write a function to prepare our data ready for the model:\n",
        "1. We load and resample the audio data by calling `batch[\"audio\"]`. As explained above, 🤗 Datasets performs any necessary resampling operations on the fly.\n",
        "2. We use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
        "3. We encode the transcriptions to label ids through the use of the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6525c478-8962-4394-a1c4-103c54cce170",
      "metadata": {
        "id": "6525c478-8962-4394-a1c4-103c54cce170"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"url\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"wylie\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13",
      "metadata": {
        "id": "70b319fb-2439-4ef6-a70d-a47bf41c4a13"
      },
      "source": [
        "We can apply the data preparation function to all of our training examples using dataset's `.map` method. The argument `num_proc` specifies how many CPU cores to use. Setting `num_proc` > 1 will enable multiprocessing. If the `.map` method hangs with multiprocessing, set `num_proc=1` and process the dataset sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b",
      "metadata": {
        "id": "7b73ab39-ffaf-4b9e-86e5-782963c6134b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8269768d19674d269db9de8360168aae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/378667 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb90e27b",
      "metadata": {},
      "outputs": [],
      "source": [
        "common_voice.push_to_hub(\"spsither/prepare_dataset_whisper_large_run1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b701171",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "# common_voice[\"train\"] = load_dataset(\"spsither/prepare_dataset_whisper_large_run1\", \"lhasa-wylie\", split=\"train\", use_auth_token=True)\n",
        "# common_voice[\"test\"] = load_dataset(\"spsither/prepare_dataset_whisper_large_run1\", \"lhasa-wylie\", split=\"test\", use_auth_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263a5a58-0239-4a25-b0df-c625fc9c5810",
      "metadata": {
        "id": "263a5a58-0239-4a25-b0df-c625fc9c5810"
      },
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7",
      "metadata": {
        "id": "a693e768-c5a6-453f-89a1-b601dcf7daf7"
      },
      "source": [
        "Now that we've prepared our data, we're ready to dive into the training pipeline.\n",
        "The [🤗 Trainer](https://huggingface.co/transformers/master/main_classes/trainer.html?highlight=trainer)\n",
        "will do much of the heavy lifting for us. All we have to do is:\n",
        "\n",
        "- Define a data collator: the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.\n",
        "\n",
        "- Evaluation metrics: during evaluation, we want to evaluate the model using the [word error rate (WER)](https://huggingface.co/metrics/wer) metric. We need to define a `compute_metrics` function that handles this computation.\n",
        "\n",
        "- Load a pre-trained checkpoint: we need to load a pre-trained checkpoint and configure it correctly for training.\n",
        "\n",
        "- Define the training configuration: this will be used by the 🤗 Trainer to define the training schedule.\n",
        "\n",
        "Once we've fine-tuned the model, we will evaluate it on the test data to verify that we have correctly trained it\n",
        "to transcribe speech in Hindi."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d230e6d-624c-400a-bbf5-fa660881df25",
      "metadata": {
        "id": "8d230e6d-624c-400a-bbf5-fa660881df25"
      },
      "source": [
        "### Define a Data Collator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04def221-0637-4a69-b242-d3f0c1d0ee78",
      "metadata": {
        "id": "04def221-0637-4a69-b242-d3f0c1d0ee78"
      },
      "source": [
        "The data collator for a sequence-to-sequence speech model is unique in the sense that it\n",
        "treats the `input_features` and `labels` independently: the  `input_features` must be\n",
        "handled by the feature extractor and the `labels` by the tokenizer.\n",
        "\n",
        "The `input_features` are already padded to 30s and converted to a log-Mel spectrogram\n",
        "of fixed dimension by action of the feature extractor, so all we have to do is convert the `input_features`\n",
        "to batched PyTorch tensors. We do this using the feature extractor's `.pad` method with `return_tensors=pt`.\n",
        "\n",
        "The `labels` on the other hand are un-padded. We first pad the sequences\n",
        "to the maximum length in the batch using the tokenizer's `.pad` method. The padding tokens\n",
        "are then replaced by `-100` so that these tokens are **not** taken into account when\n",
        "computing the loss. We then cut the BOS token from the start of the label sequence as we\n",
        "append it later during training.\n",
        "\n",
        "We can leverage the `WhisperProcessor` we defined earlier to perform both the\n",
        "feature extractor and the tokenizer operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "9919bf19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Using cached accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "Requirement already satisfied: torch>=1.10.0 in ./.env/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
            "Requirement already satisfied: packaging>=20.0 in ./.env/lib/python3.10/site-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: pyyaml in ./.env/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: psutil in ./.env/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from accelerate) (1.24.4)\n",
            "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
            "Requirement already satisfied: networkx in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: sympy in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
            "Requirement already satisfied: triton==2.0.0 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./.env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
            "Requirement already satisfied: setuptools in ./.env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (59.6.0)\n",
            "Requirement already satisfied: wheel in ./.env/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.1)\n",
            "Requirement already satisfied: cmake in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in ./.env/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in ./.env/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.21.0\n"
          ]
        }
      ],
      "source": [
        "# !pip install torch\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5",
      "metadata": {
        "id": "8326221e-ec13-4731-bb4e-51e5fc1486c5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86",
      "metadata": {
        "id": "3cae7dbf-8a50-456e-a3a8-7fd005390f86"
      },
      "source": [
        "Let's initialise the data collator we've just defined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "fc834702-c0d3-4a96-b101-7b87be32bf42",
      "metadata": {
        "id": "fc834702-c0d3-4a96-b101-7b87be32bf42"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698",
      "metadata": {
        "id": "d62bb2ab-750a-45e7-82e9-61d6f4805698"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fee1a7-a44c-461e-b047-c3917221572e",
      "metadata": {
        "id": "66fee1a7-a44c-461e-b047-c3917221572e"
      },
      "source": [
        "We'll use the word error rate (WER) metric, the 'de-facto' metric for assessing\n",
        "ASR systems. For more information, refer to the WER [docs](https://huggingface.co/metrics/wer). We'll load the WER metric from 🤗 Evaluate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0dd9fa02",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting evaluate\n",
            "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "Requirement already satisfied: dill in ./.env/lib/python3.10/site-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in ./.env/lib/python3.10/site-packages (from evaluate) (0.17.0.dev0)\n",
            "Requirement already satisfied: xxhash in ./.env/lib/python3.10/site-packages (from evaluate) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in ./.env/lib/python3.10/site-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: packaging in ./.env/lib/python3.10/site-packages (from evaluate) (23.1)\n",
            "Collecting responses<0.19\n",
            "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./.env/lib/python3.10/site-packages (from evaluate) (1.24.4)\n",
            "Requirement already satisfied: multiprocess in ./.env/lib/python3.10/site-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: pandas in ./.env/lib/python3.10/site-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in ./.env/lib/python3.10/site-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: aiohttp in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in ./.env/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: filelock in ./.env/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.env/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./.env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.0 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b22b4011-f31f-4b57-b684-c52332f92890",
      "metadata": {
        "id": "b22b4011-f31f-4b57-b684-c52332f92890"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508",
      "metadata": {
        "id": "4f32cab6-31f0-4cb9-af4c-40ba0f5fc508"
      },
      "source": [
        "We then simply have to define a function that takes our model\n",
        "predictions and returns the WER metric. This function, called\n",
        "`compute_metrics`, first replaces `-100` with the `pad_token_id`\n",
        "in the `label_ids` (undoing the step we applied in the\n",
        "data collator to ignore padded tokens correctly in the loss).\n",
        "It then decodes the predicted and label ids to strings. Finally,\n",
        "it computes the WER between the predictions and reference labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52",
      "metadata": {
        "id": "23959a70-22d0-4ffe-9fa1-72b61e75bb52"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a7183a9",
      "metadata": {},
      "source": [
        "### Load a Pre-Trained Checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "437a97fa-4864-476b-8abc-f28b8166cfa5",
      "metadata": {
        "id": "437a97fa-4864-476b-8abc-f28b8166cfa5"
      },
      "source": [
        "Now let's load the pre-trained Whisper `small` checkpoint. Again, this\n",
        "is trivial through use of 🤗 Transformers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f",
      "metadata": {
        "id": "5a10cc4b-07ec-4ebd-ac1d-7c601023594f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acd9f85f5d774b13ab90bd8e548f02d4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b12b0f2847044b4aba5a2882d8455e6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c0cebe9be684419be8563d14d418e65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-medium\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15ead5f-2277-4a39-937b-585c2497b2df",
      "metadata": {
        "id": "a15ead5f-2277-4a39-937b-585c2497b2df"
      },
      "source": [
        "Override generation arguments - no tokens are forced as decoder outputs (see [`forced_decoder_ids`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.forced_decoder_ids)), no tokens are suppressed during generation (see [`suppress_tokens`](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate.suppress_tokens)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "62038ba3-88ed-4fce-84db-338f50dcd04f",
      "metadata": {
        "id": "62038ba3-88ed-4fce-84db-338f50dcd04f"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06",
      "metadata": {
        "id": "2178dea4-80ca-47b6-b6ea-ba1915c90c06"
      },
      "source": [
        "### Define the Training Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c21af1e9-0188-4134-ac82-defc7bdcc436",
      "metadata": {
        "id": "c21af1e9-0188-4134-ac82-defc7bdcc436"
      },
      "source": [
        "In the final step, we define all the parameters related to training. For more detail on the training arguments, refer to the Seq2SeqTrainingArguments [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a",
      "metadata": {
        "id": "0ae3e9af-97b7-4aa0-ae85-20b23b5bcb3a"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-large-r1\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"wandb\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a944d8-3112-4552-82a0-be25988b3857",
      "metadata": {
        "id": "b3a944d8-3112-4552-82a0-be25988b3857"
      },
      "source": [
        "**Note**: if one does not want to upload the model checkpoints to the Hub,\n",
        "set `push_to_hub=False`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac29114-d226-4f54-97cf-8718c9f94e1e",
      "metadata": {
        "id": "bac29114-d226-4f54-97cf-8718c9f94e1e"
      },
      "source": [
        "We can forward the training arguments to the 🤗 Trainer along with our model,\n",
        "dataset, data collator and `compute_metrics` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "d546d7fe-0543-479a-b708-2ebabec19493",
      "metadata": {
        "id": "d546d7fe-0543-479a-b708-2ebabec19493"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uOrRhDGtN5S4",
      "metadata": {
        "id": "uOrRhDGtN5S4"
      },
      "source": [
        "We'll save the processor object once before starting training. Since the processor is not trainable, it won't change over the course of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "-2zQwMfEOBJq",
      "metadata": {
        "id": "-2zQwMfEOBJq"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f404cf9-4345-468c-8196-4bd101d9bd51",
      "metadata": {
        "id": "7f404cf9-4345-468c-8196-4bd101d9bd51"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112",
      "metadata": {
        "id": "5e8b8d56-5a70-4f68-bd2e-f0752d0bd112"
      },
      "source": [
        "Training will take approximately 5-10 hours depending on your GPU or the one\n",
        "allocated to this Google Colab. If using this Google Colab directly to\n",
        "fine-tune a Whisper model, you should make sure that training isn't\n",
        "interrupted due to inactivity. A simple workaround to prevent this is\n",
        "to paste the following code into the console of this tab (_right mouse click_\n",
        "-> _inspect_ -> _Console tab_ -> _insert code_)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "890a63ed-e87b-4e53-a35a-6ec1eca560af",
      "metadata": {
        "id": "890a63ed-e87b-4e53-a35a-6ec1eca560af"
      },
      "source": [
        "```javascript\n",
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton, 60000);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a55168b-2f46-4678-afa0-ff22257ec06d",
      "metadata": {
        "id": "5a55168b-2f46-4678-afa0-ff22257ec06d"
      },
      "source": [
        "The peak GPU memory for the given training configuration is approximately 15.8GB.\n",
        "Depending on the GPU allocated to the Google Colab, it is possible that you will encounter a CUDA `\"out-of-memory\"` error when you launch training.\n",
        "In this case, you can reduce the `per_device_train_batch_size` incrementally by factors of 2\n",
        "and employ [`gradient_accumulation_steps`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps)\n",
        "to compensate.\n",
        "\n",
        "To launch training, simply execute:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de",
      "metadata": {
        "id": "ee8b7b8e-1c9a-4d77-9137-1778a629e6de"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/monlamai/Documents/GitHub/stt-whisper/wandb/run-20230823_142357-symhphjh</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/monlam-ai/huggingface/runs/symhphjh' target=\"_blank\">autumn-plasma-108</a></strong> to <a href='https://wandb.ai/monlam-ai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/monlam-ai/huggingface' target=\"_blank\">https://wandb.ai/monlam-ai/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/monlam-ai/huggingface/runs/symhphjh' target=\"_blank\">https://wandb.ai/monlam-ai/huggingface/runs/symhphjh</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2fd17b73487c400eb2d94293c7f224dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.15 GiB (GPU 0; 23.64 GiB total capacity; 18.68 GiB already allocated; 530.12 MiB free; 20.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[39m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1547\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1548\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1549\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1550\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1551\u001b[0m     )\n\u001b[1;32m   1552\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/transformers/trainer.py:2693\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         scaled_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m   2692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2693\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mbackward(loss)\n\u001b[1;32m   2695\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach() \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgradient_accumulation_steps\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/accelerate/accelerator.py:1851\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1849\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1851\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/autograd/function.py:274\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    273\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/utils/checkpoint.py:157\u001b[0m, in \u001b[0;36mCheckpointFunction.backward\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(outputs_with_grad) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnone of output has requires_grad=True,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m this checkpoint() is not necessary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 157\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(outputs_with_grad, args_with_grad)\n\u001b[1;32m    158\u001b[0m grads \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(inp\u001b[39m.\u001b[39mgrad \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inp, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    159\u001b[0m               \u001b[39mfor\u001b[39;00m inp \u001b[39min\u001b[39;00m detached_inputs)\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m) \u001b[39m+\u001b[39m grads\n",
            "File \u001b[0;32m~/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.15 GiB (GPU 0; 23.64 GiB total capacity; 18.68 GiB already allocated; 530.12 MiB free; 20.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3",
      "metadata": {
        "id": "810ced54-7187-4a06-b2fe-ba6dcca94dc3"
      },
      "source": [
        "Our best WER is 32.0% - not bad for 8h of training data! We can submit our checkpoint to the [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench) on push by setting the appropriate key-word arguments (kwargs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c704f91e-241b-48c9-b8e0-f0da396a9663",
      "metadata": {
        "id": "c704f91e-241b-48c9-b8e0-f0da396a9663"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: hi, split: test\",\n",
        "    \"language\": \"hi\",\n",
        "    \"model_name\": \"Whisper large Hi - Sanchit Gandhi\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-large\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "    \"tags\": \"hf-asr-leaderboard\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "090d676a-f944-4297-a938-a40eda0b2b68",
      "metadata": {
        "id": "090d676a-f944-4297-a938-a40eda0b2b68"
      },
      "source": [
        "The training results can now be uploaded to the Hub. To do so, execute the `push_to_hub` command and save the preprocessor object we created:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "d7030622-caf7-4039-939b-6195cdaa2585",
      "metadata": {
        "id": "d7030622-caf7-4039-939b-6195cdaa2585"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/spsither/whisper-small-hi/tree/main/'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d4360d-5721-426e-b6ac-178f833fedeb",
      "metadata": {
        "id": "34d4360d-5721-426e-b6ac-178f833fedeb"
      },
      "source": [
        "## Building a Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9",
      "metadata": {
        "id": "e65489b7-18d1-447c-ba69-cd28dd80dad9"
      },
      "source": [
        "Now that we've fine-tuned our model we can build a demo to show\n",
        "off its ASR capabilities! We'll make use of 🤗 Transformers\n",
        "`pipeline`, which will take care of the entire ASR pipeline,\n",
        "right from pre-processing the audio inputs to decoding the\n",
        "model predictions.\n",
        "\n",
        "Running the example below will generate a Gradio demo where we\n",
        "can record speech through the microphone of our computer and input it to\n",
        "our fine-tuned Whisper model to transcribe the corresponding text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa",
      "metadata": {
        "id": "e0ace3aa-1ef3-45cb-933f-6ddca037c5aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c3841399e614510a12eaf81ecfcc69a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/monlamai/Documents/GitHub/stt-whisper/.env/lib/python3.10/site-packages/gradio/processing_utils.py:188: UserWarning: Trying to convert audio automatically from int32 to 16-bit int format.\n",
            "  warnings.warn(warning.format(data.dtype))\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "pipe = pipeline(model=\"spsither/whisper-large-r1\")  # change to \"your-username/the-name-you-picked\"\n",
        "\n",
        "def transcribe(audio):\n",
        "    text = pipe(audio)[\"text\"]\n",
        "    return text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Large Tibetan\",\n",
        "    description=\"Realtime demo for Tibetan speech recognition using a fine-tuned Whisper medium model.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba",
      "metadata": {
        "id": "ca743fbd-602c-48d4-ba8d-a2fe60af64ba"
      },
      "source": [
        "## Closing Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f737783-2870-4e35-aa11-86a42d7d997a",
      "metadata": {
        "id": "7f737783-2870-4e35-aa11-86a42d7d997a"
      },
      "source": [
        "In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR\n",
        "using 🤗 Datasets, Transformers and the Hugging Face Hub. For more details on the Whisper model, the Common Voice dataset and the theory behind fine-tuning, refere to the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). If you're interested in fine-tuning other\n",
        "Transformers models, both for English and multilingual ASR, be sure to check out the\n",
        "examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
