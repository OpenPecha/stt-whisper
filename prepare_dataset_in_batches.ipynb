{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95048026-a3b7-43f0-a274-1bad65e407b4",
      "metadata": {
        "id": "95048026-a3b7-43f0-a274-1bad65e407b4"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e68ea9f8-9b61-414e-8885-3033b67c2850",
      "metadata": {
        "id": "e68ea9f8-9b61-414e-8885-3033b67c2850"
      },
      "outputs": [],
      "source": [
        "! pip install datasets>=2.6.1\n",
        "! pip install git+https://github.com/huggingface/transformers\n",
        "! pip install librosa\n",
        "! pip install evaluate>=0.30\n",
        "! pip install jiwer\n",
        "! pip install gradio\n",
        "# ! pip  install datasets\n",
        "# ! pip install torch\n",
        "! pip install accelerate -U\n",
        "# ! pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad9c753",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/huggingface_hub\n",
        "!pip install wandb -qU\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0",
      "metadata": {
        "id": "b219c9dd-39b6-4a95-b2a1-3f547a1e7bc0"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "674429c5-0ab4-4adf-975b-621bb69eca38",
      "metadata": {
        "id": "674429c5-0ab4-4adf-975b-621bb69eca38"
      },
      "source": [
        "Using ðŸ¤— Datasets, downloading and preparing data is extremely simple.\n",
        "We can download and prepare the Common Voice splits in just one line of code.\n",
        "\n",
        "First, ensure you have accepted the terms of use on the Hugging Face Hub: [mozilla-foundation/common_voice_11_0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0). Once you have accepted the terms, you will have full access to the dataset and be able to download the data locally.\n",
        "\n",
        "Since Hindi is very low-resource, we'll combine the `train` and `validation`\n",
        "splits to give approximately 8 hours of training data. We'll use the 4 hours\n",
        "of `test` data as our held-out test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed601493",
      "metadata": {},
      "outputs": [],
      "source": [
        "! mkdir tsv\n",
        "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_train.tsv --output tsv/train.tsv\n",
        "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/06_val.tsv --output tsv/validation.tsv\n",
        "! curl https://d38pmlk0v88drf.cloudfront.net/tsv/05_benchmarks.tsv --output tsv/test.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "42177717",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('tsv/train.tsv', sep='\\t')\n",
        "validation_df = pd.read_csv('tsv/validation.tsv', sep='\\t')\n",
        "# test_df = pd.read_csv('tsv/test.tsv', sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35571a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.options.mode.chained_assignment = None\n",
        "# test_df['path'] = test_df['file_name'].apply(lambda x: f'/media/monlamai/SSD/wav2vec2/segments/{x.replace(\".wav\",\"\").replace(\".mp3\",\"\")}.wav')\n",
        "validation_df['path'] = validation_df['file_name'].apply(lambda x: f'/media/monlamai/SSD/wav2vec2/segments/{x.replace(\".wav\",\"\").replace(\".mp3\",\"\")}.wav')\n",
        "train_df['path'] = train_df['file_name'].apply(lambda x: f'/media/monlamai/SSD/wav2vec2/segments/{x.replace(\".wav\",\"\").replace(\".mp3\",\"\")}.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "776a51ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total: 553494, batch_size: 27674, max_batch_i: 19\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperFeatureExtractor\n",
        "from datasets import DatasetDict, Dataset\n",
        "from transformers import WhisperProcessor\n",
        "from datasets import Audio \n",
        "\n",
        "\n",
        "total = len(train_df)\n",
        "batch_size = math.floor(total * 5/100)\n",
        "\n",
        "max_batch_i = math.floor(total/batch_size) - 1\n",
        "\n",
        "print(f'total: {total}, batch_size: {batch_size}, max_batch_i: {max_batch_i}')\n",
        "\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Tibetan\", task=\"transcribe\")\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Tibetan\", task=\"transcribe\")\n",
        "\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    audio = batch[\"path\"]\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    batch[\"labels\"] = tokenizer(batch[\"wylie\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "    \n",
        "for batch_i in range(19, max_batch_i+1):\n",
        "    print(f'batch_i: {batch_i}')\n",
        "    if batch_i == max_batch_i:\n",
        "        batch_df = train_df[batch_i*batch_size:]\n",
        "    else :\n",
        "        batch_df = train_df[batch_i*batch_size:(batch_i+1)*batch_size]\n",
        "    \n",
        "    common_voice_train = Dataset.from_pandas(batch_df, split='train')\n",
        "    common_voice = DatasetDict()\n",
        "    common_voice[\"train\"] = common_voice_train\n",
        "    common_voice = common_voice.remove_columns([\"file_name\", \"uni\", \"dept\", 'url', 'grade', 'char_len', 'audio_len', 'non_word_count', 'non_bo_word_count', 'total_tokens'])\n",
        "    common_voice = common_voice.cast_column(\"path\", Audio(sampling_rate=16000))\n",
        "    common_voice['train'] = common_voice['train'].map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)\n",
        "    common_voice['train'].save_to_disk(f'/media/monlamai/SSD/whisper/prepare_dataset_train_batch_{batch_i}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312b7078",
      "metadata": {},
      "source": [
        "### run once for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50dadc28",
      "metadata": {},
      "outputs": [],
      "source": [
        "common_voice = DatasetDict()\n",
        "common_voice['test'] = Dataset.from_pandas(validation_df, split='validation')\n",
        "\n",
        "common_voice = common_voice.remove_columns([\"file_name\", \"uni\", \"dept\", 'url', 'grade', 'char_len', 'audio_len', 'non_word_count', 'non_bo_word_count', 'total_tokens'])\n",
        "common_voice = common_voice.cast_column(\"path\", Audio(sampling_rate=16000))\n",
        "\n",
        "\n",
        "common_voice['test'] = common_voice['test'].map(prepare_dataset, remove_columns=common_voice.column_names[\"test\"], num_proc=1)\n",
        "common_voice['test'].save_to_disk(f'/media/monlamai/SSD/whisper/prepare_dataset_test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f17e0372",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_i: 0\n",
            "batch_i: 1\n",
            "batch_i: 2\n",
            "batch_i: 3\n",
            "batch_i: 4\n",
            "batch_i: 5\n",
            "batch_i: 6\n",
            "batch_i: 7\n",
            "batch_i: 8\n",
            "batch_i: 9\n",
            "batch_i: 10\n",
            "batch_i: 11\n",
            "batch_i: 12\n",
            "batch_i: 13\n",
            "batch_i: 14\n",
            "batch_i: 15\n",
            "batch_i: 16\n",
            "batch_i: 17\n",
            "batch_i: 18\n",
            "batch_i: 19\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c0722807854749119bd7231729846b2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/975 shards):   0%|          | 0/507324 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2bafc35ae5c43f097b21dab28ffb71e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Saving the dataset (0/22 shards):   0%|          | 0/11296 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_from_disk, concatenate_datasets, DatasetDict\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "temp = []\n",
        "max_batch_i = 19\n",
        "for batch_i in range(max_batch_i+1):\n",
        "    print(f'batch_i: {batch_i}')\n",
        "    train_batch = load_from_disk(f'/media/monlamai/SSD/whisper/prepare_dataset_train_batch_{batch_i}')\n",
        "    temp.append(train_batch)\n",
        "\n",
        "common_voice['train'] = concatenate_datasets(temp)\n",
        "\n",
        "common_voice['test'] = load_from_disk('/media/monlamai/SSD/whisper/prepare_dataset_test')\n",
        "\n",
        "common_voice.save_to_disk('/media/monlamai/Monlam AI/spsither/whisper/prepare_dataset')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
